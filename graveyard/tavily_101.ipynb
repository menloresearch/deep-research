{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract - depth normal\n",
    "{\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/\",\n",
    "      \"raw_content\": \"Menlo Research\\n===============\\nSkip to Content\\nNEW ‚ú® AlphaSpace: A Step Closer towards Having Clumsy-less Robots\\n\\n\\nProducts\\n\\nProducts\\nJan\\nAPI Platform\\nModels\\nIchigo\\nInfra\\nOrigami\\nCommunity\\n\\nLabs\\n\\nSim Lim Square\\n\\n\\n\\nMeetups\\n\\nSingapore\\nHo Chi Minh City\\n\\n\\n\\n\\n\\nResearch\\n\\nUse Cases\\nHuman Augmentation\\nRobots\\nResearch Areas\\nReasoning\\nSpeech\\nVision\\nSpatial\\nEfficiency\\nRobotic Control\\n\\n\\n\\nBlog\\n\\nBlog\\nAlphaMaze: Teaching LLMs to think visually\\nAlphaSpace: A Step Closer towards Having Clumsy-less Robots\\nBenchmarking NVIDIA TensorRT-LLM\\nCan llama learn to listen?\\nHomebrew is now Menlo Research\\nJan is now part of something bigger: Homebrew üññ\\nMeet our first AI hires: Janice & Claire\\nüçì Ichigo: Llama learns to talk\\nLlama3 just got ears\\nJan (Menlo) is one of the 20 hottest open-source startups of 2024\\n\\n\\n\\nHandbook\\n\\nOverview\\nHandbook\\n0. What is Menlo?\\n\\n\\n\\nWhy does Menlo Exist?\\n\\n\\nWhat problem are we solving?\\n\\nWho we are building for\\nWho owns Menlo?\\n\\n\\n\\n\\n\\nHow we make Money\\n\\n\\nHow we make money\\n\\nWhat is our moat\\n\\n\\n\\n\\n\\nWho We Hire\\n\\n\\nThe Fast and the Curious\\n\\nObsessed Outliers\\nUnderdogs Welcome\\ndy/dx > y-intercept\\nMenlo is a Pro Sports Team\\n\\n\\n\\n\\n\\nMenlo's Philosophies\\n\\n\\nHumanity-aligned\\n\\nLateral Thinking using Withered Technology\\nPerpetual Optimism is a Force Multiplier\\n\\n\\n\\n6. Brand & Identity\\n\\nHow We Work\\nTeam Roster\\n\\nMenlo's Culture\\n\\nHow We Communicate\\nHow We Fight\\n\\n\\n\\nHow We Build\\n\\nGithub\\nDiscord\\nSprints\\nOKRs\\nInfra\\nAnalytics\\n\\n\\n\\nHow We Sell\\n\\nMarketing\\n\\n\\n\\nHR\\n\\n\\nHR Lifecycle\\n\\nInterviewing\\nOnboarding\\nOffboarding\\n\\n\\n\\nHR Policies\\n\\nTime off\\nSide Gigs\\nSpending\\nTeam Retreats\\nTravel\\nCareer Progression\\n1-on-1s\\nCompensation\\n\\n\\n\\n\\n\\n\\n\\nProducts 0 1\\nResearch 0 2\\nBlog 0 3\\nHandbook 0 4\\n\\n\\nAnti-Dumb\\nRobot Club\\nMenlo is an open R&D lab building the brain for robots.\\nDiscord 1.4K onlineGithub 29.3K stars\\n\\nProducts\\nWe ship experiments, some of which become products with millions of users\\nOur Products\\n\\nJan\\nGeneral AI Agent that gets things done.\\nAvailable to download:¬†1. llama3.2:3b-gguf-q2-k¬†2. llama3.2:3b-gguf-q3-kl¬†3. llama3.2:3b-gguf-q3-km (default)¬†4. llama3.2:3b-gguf-q3-ks¬†5. llama3.2:3b-gguf-q4-km¬†5. llama3.2:3b-gguf-q4-km¬†6. llama3.2:3b-gguf-q4-ksSelect a model (1-6): 2\\nCortex\\nRun AI models 100% offline\\n\\nPlayground\\nSimulation environment for Robotic research\\nHow We Work\\nMenlo is named after Menlo Park, the home of world-changing ideas.\\nTeam Handbook\\nFast & the Curious\\nWe ship insane quickly, and hire peoplewho never stop learning.\\nRemote\\nWe are from different countries, and arehumanity-aligned.\\nBuild in Public\\nWe build inpublic, together with the larger open-source ecosystem.\\n\\nHiring Humans\\nWe are a fully remote team across 10 cities.\\nCareers\\n\\n\\nThe Soul of a New Machine\\nSubscribe to our newsletter on AI research.\\nProducts\\nJanAPI Platform\\nModels\\nIchigo\\nInfra\\nOrigami\\nCommunity\\nDiscordLabsSim Lim SquareMeetupsSingaporeHo Chi Minh City\\n\\n¬©2025 Menlo Research Pte. Ltd.\",\n",
    "      \"images\": []\n",
    "    }\n",
    "  ],\n",
    "  \"failed_results\": [],\n",
    "  \"response_time\": 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Extract - depth advance\n",
    "{\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/\",\n",
    "      \"raw_content\": \"Menlo Research\\n===============\\nSkip to Content\\nNEW ‚ú® AlphaSpace: A Step Closer towards Having Clumsy-less Robots\\n\\n\\nProducts\\n\\nProducts\\nJan\\nAPI Platform\\nModels\\nIchigo\\nInfra\\nOrigami\\nCommunity\\n\\nLabs\\n\\nSim Lim Square\\n\\n\\n\\nMeetups\\n\\nSingapore\\nHo Chi Minh City\\n\\n\\n\\n\\n\\nResearch\\n\\nUse Cases\\nHuman Augmentation\\nRobots\\nResearch Areas\\nReasoning\\nSpeech\\nVision\\nSpatial\\nEfficiency\\nRobotic Control\\n\\n\\n\\nBlog\\n\\nBlog\\nAlphaMaze: Teaching LLMs to think visually\\nAlphaSpace: A Step Closer towards Having Clumsy-less Robots\\nBenchmarking NVIDIA TensorRT-LLM\\nCan llama learn to listen?\\nHomebrew is now Menlo Research\\nJan is now part of something bigger: Homebrew üññ\\nMeet our first AI hires: Janice & Claire\\nüçì Ichigo: Llama learns to talk\\nLlama3 just got ears\\nJan (Menlo) is one of the 20 hottest open-source startups of 2024\\n\\n\\n\\nHandbook\\n\\nOverview\\nHandbook\\n0. What is Menlo?\\n\\n\\n\\nWhy does Menlo Exist?\\n\\n\\nWhat problem are we solving?\\n\\nWho we are building for\\nWho owns Menlo?\\n\\n\\n\\n\\n\\nHow we make Money\\n\\n\\nHow we make money\\n\\nWhat is our moat\\n\\n\\n\\n\\n\\nWho We Hire\\n\\n\\nThe Fast and the Curious\\n\\nObsessed Outliers\\nUnderdogs Welcome\\ndy/dx > y-intercept\\nMenlo is a Pro Sports Team\\n\\n\\n\\n\\n\\nMenlo's Philosophies\\n\\n\\nHumanity-aligned\\n\\nLateral Thinking using Withered Technology\\nPerpetual Optimism is a Force Multiplier\\n\\n\\n\\n6. Brand & Identity\\n\\nHow We Work\\nTeam Roster\\n\\nMenlo's Culture\\n\\nHow We Communicate\\nHow We Fight\\n\\n\\n\\nHow We Build\\n\\nGithub\\nDiscord\\nSprints\\nOKRs\\nInfra\\nAnalytics\\n\\n\\n\\nHow We Sell\\n\\nMarketing\\n\\n\\n\\nHR\\n\\n\\nHR Lifecycle\\n\\nInterviewing\\nOnboarding\\nOffboarding\\n\\n\\n\\nHR Policies\\n\\nTime off\\nSide Gigs\\nSpending\\nTeam Retreats\\nTravel\\nCareer Progression\\n1-on-1s\\nCompensation\\n\\n\\n\\n\\n\\n\\n\\nProducts 0 1\\nResearch 0 2\\nBlog 0 3\\nHandbook 0 4\\n\\n\\nAnti-Dumb\\nRobot Club\\nMenlo is an open R&D lab building the brain for robots.\\nDiscord 1.4K onlineGithub 29.3K stars\\n\\nProducts\\nWe ship experiments, some of which become products with millions of users\\nOur Products\\n\\nJan\\nGeneral AI Agent that gets things done.\\nAvailable to download:¬†1. llama3.2:3b-gguf-q2-k¬†2. llama3.2:3b-gguf-q3-kl¬†3. llama3.2:3b-gguf-q3-km (default)¬†4. llama3.2:3b-gguf-q3-ks¬†5. llama3.2:3b-gguf-q4-km¬†5. llama3.2:3b-gguf-q4-km¬†6. llama3.2:3b-gguf-q4-ksSelect a model (1-6): 2\\nCortex\\nRun AI models 100% offline\\n\\nPlayground\\nSimulation environment for Robotic research\\nHow We Work\\nMenlo is named after Menlo Park, the home of world-changing ideas.\\nTeam Handbook\\nFast & the Curious\\nWe ship insane quickly, and hire peoplewho never stop learning.\\nRemote\\nWe are from different countries, and arehumanity-aligned.\\nBuild in Public\\nWe build inpublic, together with the larger open-source ecosystem.\\n\\nHiring Humans\\nWe are a fully remote team across 10 cities.\\nCareers\\n\\n\\nThe Soul of a New Machine\\nSubscribe to our newsletter on AI research.\\nProducts\\nJanAPI Platform\\nModels\\nIchigo\\nInfra\\nOrigami\\nCommunity\\nDiscordLabsSim Lim SquareMeetupsSingaporeHo Chi Minh City\\n\\n¬©2025 Menlo Research Pte. Ltd.\",\n",
    "      \"images\": []\n",
    "    }\n",
    "  ],\n",
    "  \"failed_results\": [],\n",
    "  \"response_time\": 2.03\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Crawl \n",
    "{\n",
    "  \"base_url\": \"https://menlo.ai/\",\n",
    "  \"results\": [\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/\",\n",
    "      \"raw_content\": \"Menlo Research\\n===============\\nSkip to Content\\nNEW ‚ú® AlphaSpace: A Step Closer towards Having Clumsy-less Robots\\n\\n\\nProducts\\n\\nProducts\\nJan\\nAPI Platform\\nModels\\nIchigo\\nInfra\\nOrigami\\nCommunity\\n\\nLabs\\n\\nSim Lim Square\\n\\n\\n\\nMeetups\\n\\nSingapore\\nHo Chi Minh City\\n\\n\\n\\n\\n\\nResearch\\n\\nUse Cases\\nHuman Augmentation\\nRobots\\nResearch Areas\\nReasoning\\nSpeech\\nVision\\nSpatial\\nEfficiency\\nRobotic Control\\n\\n\\n\\nBlog\\n\\nBlog\\nAlphaMaze: Teaching LLMs to think visually\\nAlphaSpace: A Step Closer towards Having Clumsy-less Robots\\nBenchmarking NVIDIA TensorRT-LLM\\nCan llama learn to listen?\\nHomebrew is now Menlo Research\\nJan is now part of something bigger: Homebrew üññ\\nMeet our first AI hires: Janice & Claire\\nüçì Ichigo: Llama learns to talk\\nLlama3 just got ears\\nJan (Menlo) is one of the 20 hottest open-source startups of 2024\\n\\n\\n\\nHandbook\\n\\nOverview\\nHandbook\\n0. What is Menlo?\\n\\n\\n\\nWhy does Menlo Exist?\\n\\n\\nWhat problem are we solving?\\n\\nWho we are building for\\nWho owns Menlo?\\n\\n\\n\\n\\n\\nHow we make Money\\n\\n\\nHow we make money\\n\\nWhat is our moat\\n\\n\\n\\n\\n\\nWho We Hire\\n\\n\\nThe Fast and the Curious\\n\\nObsessed Outliers\\nUnderdogs Welcome\\ndy/dx > y-intercept\\nMenlo is a Pro Sports Team\\n\\n\\n\\n\\n\\nMenlo's Philosophies\\n\\n\\nHumanity-aligned\\n\\nLateral Thinking using Withered Technology\\nPerpetual Optimism is a Force Multiplier\\n\\n\\n\\n6. Brand & Identity\\n\\nHow We Work\\nTeam Roster\\n\\nMenlo's Culture\\n\\nHow We Communicate\\nHow We Fight\\n\\n\\n\\nHow We Build\\n\\nGithub\\nDiscord\\nSprints\\nOKRs\\nInfra\\nAnalytics\\n\\n\\n\\nHow We Sell\\n\\nMarketing\\n\\n\\n\\nHR\\n\\n\\nHR Lifecycle\\n\\nInterviewing\\nOnboarding\\nOffboarding\\n\\n\\n\\nHR Policies\\n\\nTime off\\nSide Gigs\\nSpending\\nTeam Retreats\\nTravel\\nCareer Progression\\n1-on-1s\\nCompensation\\n\\n\\n\\n\\n\\n\\n\\nProducts 0 1\\nResearch 0 2\\nBlog 0 3\\nHandbook 0 4\\n\\n\\nAnti-Dumb\\nRobot Club\\nMenlo is an open R&D lab building the brain for robots.\\nDiscord 1.4K onlineGithub 29.3K stars\\n\\nProducts\\nWe ship experiments, some of which become products with millions of users\\nOur Products\\n\\nJan\\nGeneral AI Agent that gets things done.\\nAvailable to download:¬†1. llama3.2:3b-gguf-q2-k¬†2. llama3.2:3b-gguf-q3-kl¬†3. llama3.2:3b-gguf-q3-km (default)¬†4. llama3.2:3b-gguf-q3-ks¬†5. llama3.2:3b-gguf-q4-km¬†5. llama3.2:3b-gguf-q4-km¬†6. llama3.2:3b-gguf-q4-ksSelect a model (1-6): 2\\nCortex\\nRun AI models 100% offline\\n\\nPlayground\\nSimulation environment for Robotic research\\nHow We Work\\nMenlo is named after Menlo Park, the home of world-changing ideas.\\nTeam Handbook\\nFast & the Curious\\nWe ship insane quickly, and hire peoplewho never stop learning.\\nRemote\\nWe are from different countries, and arehumanity-aligned.\\nBuild in Public\\nWe build inpublic, together with the larger open-source ecosystem.\\n\\nHiring Humans\\nWe are a fully remote team across 10 cities.\\nCareers\\n\\n\\nThe Soul of a New Machine\\nSubscribe to our newsletter on AI research.\\nProducts\\nJanAPI Platform\\nModels\\nIchigo\\nInfra\\nOrigami\\nCommunity\\nDiscordLabsSim Lim SquareMeetupsSingaporeHo Chi Minh City\\n\\n¬©2025 Menlo Research Pte. Ltd.\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/research\",\n",
    "      \"raw_content\": \"On This Page\\n\\nResearch\\n\\n\\n\\nüåê Vision: Intelligence as a Continuously Expanding Network\\n\\nWe envision Open Intelligence not as a static system with fixed inputs and outputs, but rather as a dynamic, evolving network capable of integrating diverse forms of perception, reasoning, and interaction. Each new modality‚Äîwhether visual, auditory, spatial, or yet-to-be-defined‚Äîshould be able to join the framework organically, enriching the whole while benefiting from its connections.\\n\\nRather than defining intelligence by a limited set of preordained capabilities, we aim to construct a platform where novel models can emerge and co-evolve over time, unlocking unforeseen generalization abilities and creative potential.\\n\\n‚öôÔ∏è Core Pillars of Our Intelligence Framework\\n\\n1. Foundational Modalities\\n\\nThese represent the primary building blocks of our current model and are essential for real-world competency:\\n\\n2. Action & Agency\\n\\nTrue intelligence is not passive‚Äîit must act and adapt. We focus on tightly coupling perception with actionable outcomes through:\\n\\n3. Extensible Architecture for New Modalities\\n\\nOur long-term advantage lies in our ability to grow beyond today‚Äôs known modalities. The world is represented in a much richer context than simply text or images. Here are some unique modalities that demonstrate the breadth of intelligence representation:\\n\\nThis diversity of modalities highlights why any true Intelligence system must be designed for continuous expansion and integration of new forms of perception and understanding.\\n\\nüîÑ Strategic Approach: Lean, Modular, and Principles-Driven\\n\\n\\n\\n1. Why Monolithic Architectures Are Not Feasible and Why Modality Expansion Is Important\\n\\nOver the past few years, the most successful attempts at overcoming challenges in General Intelligence have focused on growing the modality of pretrained models. The core argument is that we need to leverage ‚Äúpre-train‚Äù data to enhance the model‚Äôs ability to generalize beyond its original domain.\\n\\nFor many years, research in AI has been hindered by the extreme lack of data sources. Many startups and companies have been founded‚Äîand have failed‚Äîsimply to ‚Äúmake a tool‚Äù for data collection.\\n\\nWe strongly believe that Open, General Intelligence should focus on expanding modalities and utilizing pretrained efforts. Only in this way can Open Intelligence truly be open, because efforts will build upon each other rather than being isolated, one-shot attempts.\\n\\n2. Iterative Exploration with Resource Constraints\\n\\nTo stay lean, we prioritize research that maximizes value from minimal inputs, i.e.,\\n\\nüìà Outlook: Growing Toward General Intelligence\\n\\nBy designing intelligence as a growing constellation of interdependent modalities, we transition from rigid, predefined systems to more flexible, adaptive, and ultimately more powerful solutions.\\n\\nIn this future:\\n\\nOur R&D effort isn‚Äôt about ticking off familiar modalities; it‚Äôs about building the infrastructure to support perpetual evolution. In doing so, we position ourselves not only to meet today‚Äôs challenges but to lead the next wave of breakthroughs in open-ended multimodal intelligence.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/research/speech\",\n",
    "      \"raw_content\": \"On This Page\\n\\nSpeech\\n\\nAt Menlo, we‚Äôre reimagining how AI systems process and understand speech. Our approach focuses on creating unified models that handle both speech and text natively, eliminating the need for separate processing pipelines and enabling more natural human-AI interactions.\\n\\nResearch Projects\\n\\nIchigo (Aug 2024)\\n\\nüçì Ichigo: Rethinking Speech and Language Processing\\n\\nIchigo is a unified model that processes voice and text in a shared token space‚Äîeliminating the need for separate automatic speech recognition (ASR) or text to speech (TTS) pipelines. Ichigo handles speech natively to deliver seamless, real-time interactions.\\n\\n\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/research/robots\",\n",
    "      \"raw_content\": \"On This Page\\n\\nRobots\\n\\nMenlo is experimenting with using today‚Äôs AI advances to build generally intelligent robots, with world understanding (e.g. physical, spatial) and reasoning capabilities.\\n\\nWe are focused on the ‚Äúhigher order brain‚Äù of the robot, and work with popular robot brands for the robots and their SDKs.\\n\\nGoal\\n\\nWe are working on the following:\\n\\nThis workstream also guides Menlo‚Äôs existing work on:\\n\\nLong-Term\\n\\nRobots are a long-term endeavor, and Menlo‚Äôs current view is that there we are missing big pieces. Fundamental breakthroughs needed over the next 5-10 years at both the hardware and software levels. Even then, cost needs to drop by a factor of 10x to make mass robots possible.\\n\\nHowever, the next great leap is intelligence that can act in the physical world. Embodied systems that understand and manipulate reality, will fundamentally transform the potential of human civilization.\\n\\nMenlo continues to invest in this, as we strongly believe in its transformative potential:\\n\\nWe strongly believe robots will transform human civilization‚Äôs ability to master our physical world, and allow us to move towards a Kardashev Type-1 civilization‚Äâ.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/products/origami\",\n",
    "      \"raw_content\": \"On This Page\\n\\nOrigami Cloud\\n\\nOrigami is currently an internal product. We may open it up to the public in the future.\\n\\nOrigami is our internal GPU cloud, which supports our team and affiliated open source projects.\\n\\nArchitecture\\n\\nOpen Source\\n\\nOrigami currently supports several open source projects, including:\\n\\nIf you are an open source project or contributor and would like to use Origami, please reach out to us on Discord.\\n\\nUse-Cases\\n\\nGPU Workstations\\n\\nGPU Containers\\n\\nInference Workloads\\n\\nHardware\\n\\nGPU Server\\n\\nOrigami uses custom-built GPU Servers that use commodity workstation motherboards and GPUs, that are optimized for cost efficiency and easy maintenance.\\n\\n\\n\\nWe designed the GPU for easy maintainability and upgradeability in datacenter environments.\\n\\n\\n\\nGPU Workstations\\n\\nWe currently build GPU Workstations based on Nvidia‚Äôs RTX Pro series for our team. We may offer it for sale to the public in the future.\\n\\nHistory\\n\\nMenlo has always run its own infrastructure. We started out small:\\n\\nOver time, Origami has become a internal cloud running in a datacenter, with strong uptime and reliability guarantees.\\n\\nMenlo‚Äôs original ‚ÄúGPU cloud‚Äù in May 2023\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/blog/introducing-homebrew\",\n",
    "      \"raw_content\": \"Blog Author:\\n\\nJan is now part of something bigger: Homebrew\\n\\nüëã¬†Jan is now part of something bigger: üññ¬†Homebrew\\n\\nA year ago, we started working on Jan to enable anyone to run AI on their computer. 1 million downloads later, we‚Äôve learned a lot about AI, and the issues faced at the product, software, hardware, and even datacenter levels. These challenges have inspired to us think bigger to find better solutions.\\n\\nToday, we‚Äôre becoming Homebrew, an AI research lab. We believe deeply in AI‚Äôs potential to push the human race forward, but believe there are difficult infrastructure problems that need time and commitment to solve.\\n\\nWhat we‚Äôre working on:\\n\\nAt our core, Homebrew‚Äôs culture is driven by who we are: AI enthusiasts who want to build practical, useful tools for ourselves and others. We are tinkerers: always excited about new innovations, and relentlessly finding ways to improve our product.\\n\\nTo all our users and contributors: thank you üôè. Our team is grateful for your bug reports, criticism, ideas, and feedback. We‚Äôre continually improving, and excited to ship new products in the months ahead.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/research/reasoning\",\n",
    "      \"raw_content\": \"On This Page\\n\\nReasoning Research\\n\\nAt Menlo, we‚Äôre advancing AI systems with agentic reasoning abilities. Our research focuses on developing models that can autonomously analyze challenges, adapt their strategies, and augment human capabilities.\\n\\nResearch Projects\\n\\nReZero (Apr 2025)\\n\\nüîÑ ReZero: Enhancing LLM Search Ability by Trying One-More-Time‚Äâ\\nReZero is a search-oriented language model that uses Guided Reinforcement Policy Optimization (GRPO) and a retry-reward mechanism to encourage persistent refinement of search queries until the desired result is found. Unlike typical approaches that avoid repetition, ReZero strategically employs it to improve search performance, achieving a significantly higher success rate over baseline models.\\n\\nLinks:\\n\\n\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/blog/alpha-space\",\n",
    "      \"raw_content\": \"On This Page\\n\\nBlog Author:\\n\\nResearch Authors:\\n\\nPublished April 9, 2025\\n\\nAlphaSpace: A Step Closer to Having Clumsyless Robots\\n\\nLarge Language Models (LLMs) handle text with impressive fluency, but ask one to guide a robot in\\nstacking blocks and see what happens. Bridging the gap between language understanding and physical,\\nspatial action still poses a significant hurdle. To address this, our research team at Menlo‚ÄîAlan\\nDao (Gia Tuan Dao), Dinh Bach Vu, and Bui Quang Huy‚Äîdeveloped AlphaSpace, a methodology aimed at\\nteaching existing models the nuances of navigating and manipulating objectswithin three-dimensional\\nspace via a specialized tokenization approach.\\n\\nWhat is AlphaSpace?\\n\\nThink of AlphaSpace as a spatial reasoning bootcamp for LLMs. It‚Äôs a set of techniques to help models\\nunderstand and manipulate objects in three dimensions. The goal isn‚Äôt to build a new model from scratch,\\nbut to enhance the ones we have while focusing on practical tasks like picking up, moving, or stacking\\nobjects ‚Äî the kinds of skills that will be crucial for the robots that one day will tidy our homes\\nor assemble our ikea furniture.\\n\\nThis approach moves beyond understanding language as we or LLMs do, it sits between symbolic\\nrepresentation and physical action, enabling models to operate outside of digital constraints. We\\nwant to enable the translation of a command like ‚Äúput the red cube on the blue one‚Äù (as shown below)\\ninto a series of precise movements in a real (or simulated) space.\\n\\nHow Does It Work?\\n\\nAlphaSpace‚Äôs secret sauce is a technique called semantic tokenization, which translates the details\\nof 3D space‚Äîan object‚Äôs shape, color, position (x, y coordinates), and even its height\\n(z-coordinate)‚Äîinto a structured text format that LLMs can digest. Imagine describing a ‚Äúred apple\\nat position (30, 40) with height 5‚Äù entirely through these specialized tokens. The model doesn‚Äôt ‚Äúsee‚Äù\\nthe apple, but it can reason about its properties and location based on the description provided.\\n\\nThis tokenization is hierarchical, and captures both broad locations (coarse grid) and precise spots\\n(fine grid). This structured approach allows the LLM to ‚Äúsee‚Äù through text, bypassing the need for\\ncomplex visual processing systems like traditional vision-language models often use.\\n\\nOnce the spatial scene is tokenized, the LLM undergoes a two-step training process, building on the\\nfoundation laid by our earlier work on AlphaMaze‚Äâ for maze navigation. First,\\nSupervised Fine-Tuning (SFT) teaches the model the basics using a dataset of examples containing tokenized\\nscenes paired with the correct sequence of actions. Then, Group Relative Policy Optimization (GRPO), a reinforcement\\nlearning technique, refines the model‚Äôs decision-making. This stage is where the model learns from\\ntrial and error, from within the safety of the simulated environment, to optimize its strategy for\\nbetter performance and enhanced spatial reasoning. Although GRPO was key in AlphaMaze, the current\\nAlphaSpace implementation primarily focused on SFT using synthetic reasoning data.\\n\\nPerformance and Impact\\n\\nOur researchers pitted AlphaSpace against other models using EmbodiedBench, a standard test for AI on handling\\nphysical tasks. The results were quite promising. AlphaSpace aced 10 out of 12 picking tasks and\\nmanaged 6 out of 12 stacking tasks, the performance of other models can be seen on the table below.\\nThis performance boost highlights the advantage of AlphaSpace‚Äôs structured, semantics-based approach\\ncompared to models relying more heavily on general vision-language understanding, which seem to\\nstruggle more with the nitty-gritty of precise manipulation.\\n\\nModel | Picking | Stacking | Total Accuracy (%)\\nAlphaSpace (Ours) | 10/12 | 6/12 | 66.67\\nGPT-4o | 6/12 | 3/12 | 37.5\\nClaude 3.5 Sonnet | 5/12 | 2/12 | 29.17\\nWe think AlphaSpace is a tangible step towards models that can meaningfully interact with the\\nphysical world. The potential applications are broad, from more capable domestic robots and\\nsmarter warehouse automation to more intuitive interfaces for augmented reality or even more\\nautonomous video game characters (imagine an unbounded NPC ü§î).\\n\\nThe success this experiment underscores the power of symbolic reasoning and structured spatial\\nrepresentation for specific manipulation tasks, offering a different, potentially more efficient path\\nthan compute-heavy vision-centric models for certain applications. It suggests that sometimes,\\ntranslating the world into the right ‚Äúlanguage‚Äù is more effective than trying to make the model see it directly.\\n\\nLooking Ahead\\n\\nWhile the benchmark results are encouraging, AlphaSpace is still learning to walk, let alone run, in\\nthe real world. The current methodology shines in controlled, simulated environments with clearly\\ndefined objects and tasks. Adapting it to the chaos of dynamic, unpredictable real-world settings‚Äîwhere\\nobjects move, lighting changes, and unexpected obstacles appear is the next major hurdle.\\n\\nResearch is ongoing to make AlphaSpace more robust and adaptable. Exploring hybrid models that combine\\nAlphaSpace‚Äôs symbolic strengths with lightweight visual processing, or integrating reinforcement learning\\nfor real-time adaptation, are promising directions. Scaling its capabilities to handle more complex\\nsequences of actions, multi-robot collaboration, or tasks involving object rotation also remains future work.\\n\\nFurthermore, the initial project scope was narrowed, focusing on only two types of manipulation tasks\\n(picking and stacking) from the benchmark suite, leaving other spatial challenges unevaluated. More\\ncomprehensive testing is needed to fully map its strengths and weaknesses.\\n\\n\\n\\nConclusion\\n\\nAlphaSpace isn‚Äôt grabbing the full potential of LLM applications in the robotics and AI space yet, but it\\nis a step in the right direction. As a methodology, it shows that by cleverly translating 3D space into a\\ntoken LLMs understand and combining this with targeted training, we can imbue these models with\\nsurprisingly effective spatial reasoning skills for manipulation. Its strong benchmark performance,\\nparticularly against leading generalist models, highlights the potential of specialized, structure-aware\\napproaches. By focusing on symbolic representation and reasoning, AlphaSpace offers a lightweight yet powerful\\npathway towards models that can not only talk the talk but also walk the walk‚Äîor at least, stack the block\\ncorrectly. The journey to fully embodied AI is long, but AlphaSpace offers a helping hand.\\n\\nHere are some resources additional resouces to learn more about AlphaSpace:\\n\\nAcknowledgements\\n\\nWe would like to acknowledge the creators of the DeepSeek and Qwen models for providing the foundation\\nfor our work. We also thank the authors of the EmbodiedBench benchmark, which was valuable for evaluating\\nAlphaSpace‚Äôs capabilities.\\n\\nOpen Call\\n\\nWe‚Äôre calling on researchers to experiment & build-in-public with us. Join the\\n#research‚Äâ channel in Discord.\\n\\nWe believe that collaborative, open research can accelerate progress in this exciting field. Whether you‚Äôre\\nan experienced researcher or an enthusiastic newcomer, your contribution could be valuable.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/team\",\n",
    "      \"raw_content\": \"On This Page\\n\\nTeam Roster\\n\\nMenlo operates in small teams that optimize for Autonomy, Mastery, and Purpose‚Äâ.\\n\\nMenlo is currently 25 people (2 part-time).\\n\\nProducts\\n\\nJan\\n\\nTeam | Humans | Robots\\nJan | Louis, Sang, Faisal, Akarshan | \\nAPI Platform | Ramon, Thien | \\nQA + Release Manager | David | \\nCommunity |  | Janice\\nModel Training\\n\\nTeam | Humans | Robots\\nResearch | Alan, Bach, Norapat, Thinh | \\nSpeech | Jiaqi | \\nOrigami\\n\\nTeam | Humans | Robots\\nOrigami | Hien, Minh Doug | \\neCommerce | Emre | \\nNext Bets\\n\\nTeam | Humans | Robots\\nHuman Augmentation | Rach, Lea, Rachel, DanielC | \\nRobots | Selim, Alex, Jasshan, Yuuki, Bing Gwong | \\nBusiness Operations\\n\\nBD & Marketing\\n\\nTeam | Humans | Robots\\nMarketing | Emre, Ronny | \\nFinance & HR\\n\\nTeam | Humans | Robots\\nFinance | Yeowhua | \\nHR | Nicole, Yeowhua | \\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/research/spatial\",\n",
    "      \"raw_content\": \"On This Page\\n\\nSpatial\\n\\nAt Menlo, we‚Äôre pioneering new approaches to spatial intelligence in AI systems. Our research focuses on developing models that can understand and reason about spatial relationships, enabling more sophisticated interactions with the physical world.\\n\\nResearch Projects\\n\\nAlphaSpace (Apr 2025)\\n\\nü§ñ AlphaSpace: A Step Closer to Having Clumsyless Robots\\n\\nAlphaSpace is a cutting-edge framework that enhances spatial intelligence in robotics, enabling robots to perform complex tasks with precision and adaptability. By integrating advanced spatial reasoning capabilities, AlphaSpace empowers robots to navigate, plan, and execute actions in dynamic environments, achieving a 66.67% accuracy rate in manipulation tasks.\\n\\n\\n\\nAlphaMaze (Mar 2025)\\n\\nüß© AlphaMaze: Teaching LLMs to Think Visually\\n\\nAlphaMaze is an advanced decoder-only large language model designed to excel in visual reasoning tasks, particularly in solving spatial challenges like mazes. Built on a 1.5 billion parameter Qwen model, it demonstrates impressive capabilities in handling complex visual reasoning tasks through innovative techniques like GRPO.\\n\\n\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/research/vision\",\n",
    "      \"raw_content\": \"On This Page\\n\\nVision\\n\\nAt Menlo, we‚Äôre pushing the boundaries of computer vision by developing innovative approaches that bridge the gap between 2D and 3D understanding. Our research focuses on creating models that can interpret and reason about visual information in ways that are more aligned with human perception.\\n\\nResearch Projects\\n\\nVoxRep (Apr 2025)\\n\\nüéØ VoxRep: Enhancing 3D Spatial Understanding in 2D Vision-Language Models\\n\\nVoxRep is a novel framework that enables standard Vision-Language Models to interpret structured 3D voxel data, facilitating richer spatial comprehension vital for advanced AI systems. By processing 3D voxel space through systematic 2D slices, VoxRep demonstrates how general-purpose 2D models can effectively learn 3D representations, opening new possibilities for robotics, autonomous navigation, and virtual reality applications.\\n\\nLinks:\\n\\n\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/blog/benchmarking-nvidia-tensorrt-llm\",\n",
    "      \"raw_content\": \"On This Page\\n\\nBenchmarking NVIDIA TensorRT-LLM\\n\\nJan now supports NVIDIA TensorRT-LLM‚Äâ in addition to llama.cpp‚Äâ, making Jan multi-engine and ultra-fast for users with Nvidia GPUs.\\n\\nWe‚Äôve been excited for TensorRT-LLM for a while, and had a lot of fun implementing it‚Äâ. As part of the process, we‚Äôve run some benchmarks, to see how TensorRT-LLM fares on consumer hardware (e.g. 4090s‚Äâ, 3090s‚Äâ) we commonly see in the Jan‚Äôs hardware community‚Äâ.\\n\\nGive it a try! Jan‚Äôs TensorRT-LLM extension is available in Jan v0.4.9 and up (see more). We precompiled some TensorRT-LLM models for you to try: Mistral 7b, TinyLlama-1.1b, TinyJensen-1.1b üòÇ\\n\\nMistral 7b\\n\\nTinyLlama-1.1b\\n\\nTinyJensen-1.1b\\n\\nBugs or feedback? Let us know on GitHub‚Äâ or via Discord‚Äâ.\\n\\nAn interesting aside: Jan actually started out in June 2023 building on\\nNVIDIA FastTransformer‚Äâ, the\\nprecursor library to TensorRT-LLM. TensorRT-LLM was released in September\\n2023, making it a very young library. We‚Äôre excited to see its roadmap\\ndevelop!\\n\\nKey Findings\\n\\n\\n\\nTensorRT-LLM was:\\n\\nWhy TensorRT-LLM?\\n\\nTensorRT-LLM‚Äâ is Nvidia‚Äôs open-source inference library that incorporates Nvidia‚Äôs proprietary optimizations beyond the open-source cuBLAS‚Äâ library.\\n\\nAs compared to llama.cpp‚Äâ, which today dominates Desktop AI as a cross-platform inference engine, TensorRT-LLM is highly optimized for Nvidia GPUs. While llama.cpp compiles models into a single, generalizable CUDA ‚Äúbackend‚Äù‚Äâ that can run on a wide range of Nvidia GPUs, TensorRT-LLM compiles models into a GPU-specific execution graph‚Äâ that is highly optimized for that specific GPU‚Äôs Tensor Cores, CUDA cores, VRAM and memory bandwidth.\\n\\nTensorRT-LLM is typically used in datacenter-grade GPUs, where it produces a face-melting 10,000 tokens/s‚Äâ on NVIDIA H100 Tensor Core GPUs‚Äâ. We were curious for how TensorRT-LLM performs on consumer-grade GPUs, and gave it a spin.\\n\\nLlama.cpp | TensorRT-LLM\\nBaseline | Blazing fast (30-70% faster)\\nCompile once, run cross-platform | Compiled and highly optimized for specific GPU architecture\\nGeneralizable and Portable | Highly-optimized\\nModel compiles tosingle, generalizable CUDA ‚Äúbackend‚Äù | Model compiles toGPU-specific execution graph\\nExperiment Setup\\n\\nWe ran the experiment using standardized inference requests in a sandboxed environment:\\n\\nint4\\n\\nbatch_size 1\\n\\ninput_len 2048\\n\\noutput_len 512\\n\\nnvidia-smi\\n\\nWe picked a batch size of\\n1‚Äâ\\nto simulate realistic real-world use cases, as Jan users are likely to make\\none inference request at a time. We also used a input_len of 2048 and\\noutput_len of 512 to simulate a typical chatbot use case.\\n\\ninput_len\\n\\noutput_len\\n\\nFurther Research: We found GPU VRAM Utilization to be difficult to\\nmeasure, with nvidia-smi on Windows producing a different result from\\nWindows Task Manager‚Äôs Resource Monitor for NVIDIA GPUs. After some\\nresearch‚Äâ,\\nwe have gone with nvidia-smi‚Äôs measurement.\\n\\nnvidia-smi\\n\\nnvidia-smi\\n\\nHardware Selection\\n\\nWe chose the following GPUs based on our users‚Äô preferences:\\n\\nNVIDIA GPU | VRAM (GB) | CUDA Cores | Tensor Cores | Memory Bus Width (bit) | Memory Bandwidth (GB/s) | Connection (GB/s)\\nGeForce RTX 4090 (Ada) | 24 | 16,384 | 512 | 384 | ~1000 | PCIe4.0 x16 (~32)\\nGeForce RTX 3090 (Ampere) | 24 | 10,496 | 328 | 384 | 935.8 | PCIe4.0 x16 (~32)\\nGeForce RTX 4070 Laptop GPU (Ada) | 8 | 7680 | 144 | 192 | 272 | PCIe4.0 x4 (~8)\\nGeForce RTX 4090 eGPU (Ada) | 24 | 16,384 | 512 | 384 | ~1000 | Thunderbolt 3 connected to a USB4 USB-C port (~1.25-5?)\\nllama.cpp Setup\\n\\nMistral-7b-q4_k_m\\n\\nGGUF\\n\\nngl\\n\\n100\\n\\nNote: ngl is the abbreviation of Number of GPU Layers with the range from\\n0 as no GPU acceleration to 120 as full on GPU for some big models.\\n\\nngl\\n\\nNumber of GPU Layers\\n\\n0\\n\\n120\\n\\nTensorRT-LLM Setup\\n\\nMistral-7b-int4 AWQ\\n\\nfree_gpu_memory_fraction\\n\\nResults\\n\\nNVIDIA GeForce RTX 4090 GPU\\n\\nJan is built on this Dual-4090 workstation, which recently got upgraded to a nice case\\n\\nThe original case (or lack thereof) for our Dual-4090 cluster, as posted on r/localllama\\n\\nHardware Details\\n\\nModel Details\\n\\nFor this test, we used Jan‚Äôs Dual-4090 workstation‚Äâ, which our engineers timeshare to build Jan.\\n\\nThe NVIDIA GeForce RTX 4090‚Äâ is the latest top-of-the-line desktop GPU, with an MSRP of $1,599, and uses the Ada architecture. It has a ~1000 GB/s memory bandwidth within VRAM, and a PCIe4 x16 lane (~32 GB/s) between the GPU and the CPU.\\n\\nMetrics | GGUF (using CPU) | GGUF (using GPU) | TensorRT-LLM | How TensorRT-LLM Compares\\nThroughput (token/s) | 14.0 | 100.43 | 170.63 | ‚úÖ 69.89% faster\\nMax GPU Utilization (%) | N/A | 83.50 | 88.50 | 5.99% more\\nMax VRAM Utilization (%) | N/A | 64 | 72.1 | 12.66% more\\nAvg RAM Used (GB) | 0.611 | 7.105 | 4.98 | ‚úÖ 29.88% less\\nDisk Size (GB) | 4.07 | 4.06 | 3.05 | ‚úÖ 24.88% smaller\\nTensorRT-LLM was almost 70% faster than llama.cpp by building the model for the GeForce RTX 4090 GPU‚Äôs Ada architecture for optimal graph execution, fully utilizing the 512 Tensor Cores, 16,384 CUDA cores, and 1,000 GB/s of memory bandwidth.\\n\\nThe intuition for why llama.cpp is slower is because it compiles a model into a single, generalizable CUDA ‚Äúbackend‚Äù‚Äâ that can run on many NVIDIA GPUs. Doing so requires llama.cpp to sacrifice all the optimizations that TensorRT-LLM makes with its compilation to a GPU-specific execution graph.\\n\\nNVIDIA GeForce RTX 3090 GPU\\n\\nOur 3090 Machine, now used by one of our engineers to build Jan\\n\\nHardware Details\\n\\nModel Details\\n\\nThe NVIDIA‚Äôs GeForce RTX 3090‚Äâ is a popular desktop GPU, and retails for approximately $1,500 (as of April 24). It uses the NVIDIA Ampere architecture. As compared to its successor GeForce RTX 4090, it has 33% fewer CUDA cores (10,496) and Tensor Cores (328) and 7% less memory bandwidth (~930 GB/s).\\n\\nMetrics | GGUF (using CPU) | GGUF (using GPU) | TensorRT-LLM | How TensorRT-LLM Compares\\nThroughput (token/s) | 11.42 | 88.70 | 144.19 | ‚úÖ 62.57% faster\\nMax GPU Utilization (%) | N/A | 80.40 | 89.10 | 10.82% more\\nMax VRAM Utilization (%) | N/A | 66.80 | 76.20 | 14.07% more\\nAvg RAM Used (GB) | 0.611 | 2.60 | 0.98 | 62.41%% less\\nDisk Size (GB) | 4.07 | 4.06 | 3.05 | ‚úÖ 24.88% smaller\\nInterestingly, the GeForce RTX 3090 was only 16.6% slower compared with the GeForce RTX 4090. On TPS, TensorRT-LLM outperformed llama.cpp by 62.57%. Curiously, it also used negligible RAM for subsequent inference requests after the initial model warmup.\\n\\nNVIDIA GeForce RTX 4070 Laptop GPU\\n\\n\\n\\nHardware Details\\n\\nModel Details\\n\\nQ4_K_M\\n\\nWe also benchmarked an NVIDIA GeForce RTX 4070 Laptop GPU with 8gb of VRAM, which is a popular configuration among Jan users. Laptop GPUs are less powerful than their desktop counterparts, as they trade portability for reduced energy consumption and thermal constraints.\\n\\nMetrics | GGUF (using CPU) | GGUF (using GPU) | TensorRT-LLM | Difference on GPU\\nThroughput (token/s) | 11.57 | 39.70 | 51.57 | ‚úÖ 29.9% faster\\nMax GPU Utilization (%) | N/A | 80.00 | 84.67 | 5.83% more\\nMax VRAM Utilization (%) | N/A | 72.78 | 81.22 | 11.60% more\\nAvg RAM Used (GB) | 4.49 | 4.44 | 1.04 | ‚úÖ 76.55%% less\\nDisk Size (GB) | 4.07 | 4.06 | 3.05 | ‚úÖ 24.88% smaller\\nTensorRT-LLM on the laptop dGPU was 29.9% faster in tokens per second throughput than llama.cpp, but significantly slower than the desktop GPUs.\\n\\nThe intuition for this is fairly simple: the GeForce RTX 4070 Laptop GPU has 53.1% fewer CUDA cores and Tensor Cores (compared to the 4090), and less VRAM (8gb vs. 24gb). This reduces the surface area for GPU-specific optimizations for TensorRT-LLM.\\n\\nThe GeForce RTX 4070 Laptop GPU is also ~70% slower than the GeForce RTX 4090 desktop GPU, showing the hardware effect of less electricity draw, less VRAM, and thermal constraints on inference speed.\\n\\nLaptop with NVIDIA GeForce RTX 4090 eGPU\\n\\n\\n\\nHardware Details\\n\\nModel Details\\n\\nQ4_K_M\\n\\nOur last benchmark was to experiment with an Asus RTX 4090 eGPU‚Äâ, that was connected via a Thunderbolt 3 port‚Äâ to the Razer Blade 14‚Äôs USB4 port‚Äâ. Theoretically, the results should be fairly similar to the GeForce RTX 4090 desktop GPU as they have identical underlying GPUs, but with very different connection speeds.\\n\\nWe thought it would be an interesting to see how TensorRT-LLM handles a 68.4% reduction in communication bandwidth between the CPU and GPU:\\n\\nHelp Needed: We are actually not 100% sure what the actual Connection\\nSpeed is between Thunderbolt 3 to\\nUSB4‚Äâ, as\\nthe actual speed seems different from the advertised speed of 5 GB/s. There\\nseem to be a lot of factors involved, including the actual cable itself. We‚Äôd\\nlove if someone in the community can guide us!\\n\\nOverall, we used mid-to-high-end NVIDIA desktop GPUs for our tests, as TensorRT-LLM‚Äôs performance enhancements are most apparent on bigger VRAMs. For users with lower-spec machines, llama.cpp is better.\\n\\nMetrics | GGUF (using CPU) | GGUF (using GPU) | TensorRT-LLM | Difference on GPU\\nThroughput (token/s) | 11.56 | 62.22 | 104.95 | ‚úÖ 68.66% faster\\nMax VRAM Utilization (%) | 0 | 65 | 99 | 52.31% more\\nRAM Used (GB) | 0.611 | 5.38 | 4.11 | ‚úÖ 23.61% less\\nDisk Size (GB) | 4.07 | 4.06 | 3.05 | ‚úÖ 24.88% smaller\\nThe Thunderbolt 3 eGPU had a 38.5% lower tokens/s as compared to the PCIe4.0 x16 connected GPU. But the % speedup vs. llama.cpp was similar, at around 69%.\\n\\nInterestingly, the VRAM used with the eGPU was variably higher. Our hypothesis is that the slower communication bandwidth results in more VRAM being allocated, as memory is released mostly slowly as well.\\n\\nConclusion\\n\\nToken Speed\\n\\n\\n\\nThroughput (Higher is Better) | TensorRT-LLM | Llama.cpp | % Difference\\nGeForce RTX 4090 desktop GPU | ‚úÖ 170.63t/s | 100.43t/s | 69.89% faster\\nGeForce RTX 3090 desktop GPU | ‚úÖ 144.19t/s | 88.70t/s | 62.57% faster\\nGeForce RTX 4090 eGPU | ‚úÖ 104.95t/s | 62.22t/s | 68.66% faster\\nGeForce RTX 4070 Laptop GPU | ‚úÖ 51.57t/s | 39.70t/s | 29.90% faster\\nLaptop AMD Ryzen‚Ñ¢ 9 8945HS, 8C/16T | (Not supported) | ‚úÖ 11.57t/s | \\nMax VRAM Utilization\\n\\nAverage VRAM utilization % | TensorRT-LLM | Llama.cpp | % Difference\\nGeForce RTX 4090 desktop GPU | 72.10 | 64.00 | 12.66% more\\nGeForce RTX 3090 desktop GPU | 76.20 | 66.80 | 14.07% more\\nGeForce RTX 4070 Laptop GPU | 81.22 | 72.78 | 11.06% more\\nGeForce RTX 4090 eGPU | N/A | N/A | N/A\\nAWQ INT4\\n\\nq4_k_m\\n\\nMax RAM Usage\\n\\nMax RAM utilization | TensorRT-LLM | Llama.cpp | % Difference\\nGeForce RTX 4090 desktop GPU | ‚úÖ 4.98 | 7.11 | ‚úÖ 29.88% less\\nGeForce RTX 3090 desktop GPU | ‚úÖ 0.98 | 2.60 | ‚úÖ 62.41% less\\nGeForce RTX 4070 Laptop GPU | ‚úÖ 1.04 | 4.44 | ‚úÖ 76.55%% less\\nGeForce RTX 4090 eGPU | ‚úÖ 4.11 | 5.38 | ‚úÖ 23.61% less\\nTensorRT-LLM uses a lot less Max RAM vs. llama.cpp on slower connection (PCIe 3.0 or Thunderbolt 3) due to better memory management and efficient delegation to VRAM. On faster connection, it‚Äôs at least equal to llama.cpp.\\n\\nCompiled Model Size and Number of Files\\n\\nModel size (Lower is better) | TensorRT-LLM AWQ int4 | Llama.cpp GGUF Q4 | % Difference\\nMistral 7B | ‚úÖ 3.05GB | 4.06GB | ‚úÖ 24.88% smaller\\nConvenience\\n\\nAccessibility\\n\\nFinal Notes\\n\\nOur benchmarking is not perfect. We evaluated over a dozen tools (llmperf‚Äâ, psutil‚Äâ, gpustat‚Äâ, native utilities, and more) and found that everyone measures TPS, common metrics differently. We eventually settled on using our own tools in Jan, which are consistent across any inference engine and hardware. As for runtime parameters, we went with default settings, likely representing the typical user experience.\\n\\nWe also did not overclock for this benchmark , as it is not a default setting for most users. But we‚Äôve measured in our tests that TensorRT-LLM can go even faster with a few tweaks. We see this as a pretty exciting future direction.\\n\\nHow to Overclock: We found an additional 15% increase in performance\\nwith TensorRT-LLM by: - Enabling\\nXMP\\n\\n3600\\n\\n5600\\n\\nWe‚Äôre also publishing the underlying raw experimental data‚Äâ, and would encourage the community to scrutinize and help us improve.\\n\\nSpecial thanks to Asli Sabanci Demiroz, Annamalai Chockalingam, Jordan Dodge from Nvidia, and Georgi Gerganov from llama.cpp for feedback, review and suggestions.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/research/robotic-control\",\n",
    "      \"raw_content\": \"On This Page\\n\\nRobotic Control Research\\n\\nAt Menlo, we‚Äôre pushing the boundaries of robotic control through innovative AI approaches. Our research focuses on developing novel frameworks that enable more intuitive, efficient, and robust control of robotic systems, particularly in complex manipulation tasks.\\n\\nResearch Projects\\n\\nPoseless (Mar 2025)\\n\\nü§ñ PoseLess: Vision-Based Robotic Hand Control Without Pose Estimation\\n\\nPoseLess is a groundbreaking framework that revolutionizes robot hand control by eliminating the need for explicit pose estimation. By directly mapping 2D images to joint angles using projected representations, PoseLess enables more intuitive and efficient control of robotic hands. This innovative approach opens new possibilities for robotics, prosthetics, and human-robot interaction, achieving robust performance through synthetic data training and cross-morphology generalization.\\n\\nLinks:\\n\\n\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/about\",\n",
    "      \"raw_content\": \"On This Page\\n\\nAbout Menlo\\n\\nMenlo is an open R&D lab in pursuit of General Intelligence, that achieves real-world impact through agents and robots.\\n\\nMenlo Park‚Äâ, 1876 ‚Äî the world‚Äôs first invention lab. A place where ideas became tools.\\n\\nMenlo Park‚Äâ, 1876 ‚Äî the world‚Äôs first invention lab. A place where ideas became tools.\\n\\nWe believe general intelligence will come from systems that learn, act, and improve in the world across form factors, environments, and interfaces. We‚Äôre making general intelligence legible, hackable, and grounded in reality.\\n\\nWhy We Exist\\n\\nBreakthrough technologies need more than raw discovery. They need invention labs; places where theory meets practice, and where ideas become tools.\\n\\nIn 1876, Thomas Edison built Menlo Park‚Äâ, the world‚Äôs first true invention lab. It wasn‚Äôt just about chasing patents, it was about turning electricity into something useful, accessible, and world-changing.\\n\\nToday, intelligence is that new electricity. Menlo Research exists to make it useful, open, and humanity-aligned.\\n\\nWhat We Believe\\n\\nWe believe general intelligence will emerge from systems that learn, adapt, and improve through use.\\n\\nWe see a future where:\\n\\nWe are optimistic that intelligence can be a force for good, if we all build it with care and in the open.\\n\\nWhere We‚Äôre Headed\\n\\nWe‚Äôre building the full stack of general intelligence from consumer-facing apps and APIs to infrastructure, hardware, and eventually model training:\\n\\nLike Edison‚Äôs Menlo Park, we‚Äôre building a lab where today‚Äôs ideas become tomorrow‚Äôs tools. Like every true invention lab, we don‚Äôt pretend to have all the answers. We build, test, learn, and iterate ‚Äî until intelligence becomes something people can trust, own, and use.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/products/jan\",\n",
    "      \"raw_content\": \"On This Page\\n\\nJan\\n\\nJan‚Äôs website can be found at https://jan.ai\\n\\n\\n\\nAbout\\n\\nJan is a General AI Agent that gets things done for you:\\n\\nJan has been downloaded over 3 million times and is available on Desktop (and soon Mobile).\\n\\nBuild in Public\\n\\nJan is built in public as an open-source product, with an active community:\\n\\n#jan\\n\\nVision\\n\\nSelf-Driving Computer\\n\\nAgents are not only going to change how everyone interacts with computers. They‚Äôre also going to upend the software industry, bringing about the biggest revolution in computing since we went from typing commands to tapping on icons. - Bill Gates\\n\\nJan represents an evolution: from computers you operate, to computers that operate themselves:\\n\\nUser Ownership\\n\\nJan is an open-source alternative to closed-source consumer AI apps like ChatGPT, Claude.\\n\\nSocials\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/blog/llama3-just-got-ears\",\n",
    "      \"raw_content\": \"On This Page\\n\\nBlog Authors:\\n\\nPublished August 23, 2024\\n\\nLlama3 just got ears\\n\\nWe‚Äôre excited to share llama3-s v0.2‚Äâ, our latest multimodal checkpoint with improved speech understanding.\\n\\nDemo\\n\\nA realtime demo of Llama3-Speech (23th Aug 2024 checkpoint): the MLLM listens to human speech and responds in text\\n\\nLlama3-s v0.2 consistently performs across multiple Speech Understanding benchmarks (see Results). While more analysis is needed, we‚Äôre excited to share this progress with the community and get feedback.\\n\\nSpeech Understanding\\n\\nYou can try it for yourself:\\n\\n*Inference may slow/queued due to shared compute\\n\\n*For this round, please ask questions in English and keep them under 10 seconds long. This is due to our model‚Äôs limitation in being trained on audio prompts with fewer than 500 tokens, which we plan to address in a future update.\\n\\nThis post shares results and methodology behind an Aug 20th checkpoint. As always, this is just the beginning, and we need your ideas to push this research further.\\n\\nüí° We invite you to join¬†llama3-s‚Äâ: an ongoing, open-source, and open-data research experiment teaching llama3 to listen. See motivation‚Äâ.\\n\\nArchitecture\\n\\nIn a previous post‚Äâ, we shared llama3-s v0.1‚Äâ, an early-fusion experiment where we instruct-tuned llama3 on encodec‚Äôs‚Äâ acoustic tokens [fig 1]. While we observed some transitivity between the LLM‚Äôs text and new audio tokens, there were imminent limitations like the lack of generalization to non-synthetic voices, among other issues‚Äâ.\\n\\nFig 1. our previous acoustic tokens early-fusion experiment\\n\\nFig 2: the current approach is early-fusion with semantic tokens\\n\\nFor llama3-s v0.2, we adapted llama3.1 using early-fusion with semantic tokens, inspired by community feedback‚Äâ [fig 2]. Our goal is to leverage the benefits of semantic tokens, such as simplicity, better compression, and consistent speech-feature extraction, as demonstrated by WhisperVQ‚Äâ. We can always scale up to a hybrid approach and reintroduce acoustic features as needed, given more data and compute resources.\\n\\nYou can learn more about our comparison of semantic and acoustic tokens here.\\n\\nTraining\\n\\nStage 1: pre-training on real speech\\n\\nWe found it useful to pre-train llama3.1 on continuous speech, through rough ablation experimentation. This enhanced llama3‚Äôs ability to generalize across semantic tokens.\\n\\nData: We used the MLS-10k dataset‚Äâ (10 hours of unlabeled, multilingual human speech courtesy of OpenSLR) to pre-train llama3.1 8b on next token prediction (code here‚Äâ).\\n\\n\\n\\nTraining: The pretraining totaled 5k steps and took over 30 hours*. We used Torchtune‚Äôs‚Äâ fully sharded data parallels, an AdamW Fused optimizer, along with the following parameters:\\n\\nParameter | Continual Training\\nEpoch | 1\\nGlobal batch size | 80\\nLearning Rate | 2e-4\\nLearning Scheduler | LambdaLR with warmup\\nOptimizer | AdamW Fused\\nWarmup Steps | 20\\nWeight Decay | 0.01\\nGradient Checkpointing | Full\\nMax length | 512\\nPrecision | bf16\\nThe learning rate schedule is as follows, starting with a relatively high LR for sufficient warmup.\\n\\n\\n\\nLoss: After 5000 steps, loss converged at just below 2, at which point we simply moved onto the next stage.\\n\\n\\n\\nCompute: We used a single 10x RTX A6000 node to train. We actually own and operate our own A6000‚Äôs node that we‚Äôve fondly named ‚ÄúBoba‚Äù. So, to get a rough cost calculation for this stage, let us assume a higher end rate of USD0.80 per GPU, totaling $240 for the pre-train.\\n\\n\\n\\nMMLU Eval: We measured MMLU at this stage to get a sense of degradation. 0-shot MMLU dropped from 0.63 ‚Üí 0.46, a 30% decrease that we hoped to recover in the subsequent stage.\\n\\nStage 2: instruct tuning on a mixture of synthetic data\\n\\nFor the second stage of training, we instruct-tuned llama3 with interleaving synthetic data.\\n\\nData: We use a synthetically generated speech dataset‚Äâ. This speech data is then semantically encoded with WhisperVQ‚Äâ from WhisperSpeech‚Äâ. This dataset was then interleaved to have 70% speech instruction prompts and 30% speech transcription prompts.\\n\\n\\n\\nTraining: ¬†The instruct tuning was done with fsdp2, mixed-precision, with the final weights in bf16. We used the AdamW Fused¬†optimizer, a global batchsize of 128 (mini-batches of 2-4), a 0.5e-4 LR, and Cosine learning scheduler. You can find the full steps to reproduce our training here‚Äâ.\\n\\nParameter | Continual Training\\nEpoch | 1\\nGlobal batch size | 128\\nLearning Rate | 0.5e-4\\nLearning Scheduler | Cosine with warmup\\nOptimizer | AdamW Fused\\nWarmup Steps | 73\\nWeight Decay | 0.005\\nGradient Checkpointing | Full\\nMax length | 1024\\nPrecision | bf16\\nCompute: The training took place over 32 hours on 8x H100s, spanning 5 epochs at 6 hours & 7261 steps per epoch. At 2.20perH100perhour,weestimatethisruntohavecosted2.20 per H100 per hour, we estimate this run to have costed 2.20perH100perhour,weestimatethisruntohavecosted563, not including several failed runs due to troubleshooting.\\n\\nModel Flops Utilization (MFU) per step is around 20-25%‚Äâ, which is hugely optimizable. It‚Äôs also worth mentioning that we intentionally overtrained at this stage to run some grokking‚Äâ experiments.\\n\\nIn total, both stages of training was achievable under 600,withtheentireexperimentcomingunder600, with the entire experiment coming under 600,withtheentireexperimentcomingunder2800, accounting for various data pipelines and failed runs due to bugs and infrastructure interruptions.\\n\\nResults\\n\\nWe found epoch 3 to be performant and is our current demo checkpoint.\\n\\n\\n\\nAudioBench Eval: AudioBench‚Äâ is a June 2024 benchmark designed to evaluate audio large language models (AudioLLMs). It measures speech capabilities, in addition to ASR, transcription, etc., through a compilation of many open datasets.\\n\\nModel Bench | Open-hermes Instruction Audio(GPT-4-O judge 0:5) | Alpaca Instruction Audio(GPT-4-O judge 0:5) | Librispeech clean v2(ASR) (WER score)\\nLlama3.1-s-v2-epoch-1 | 3.02 | 2.87 | 94.66%\\nLlama3.1-s-v2-epoch-2 | 3.0 | 3.22 | 60.80%\\nLlama3.1-s-v2-epoch-3 | 3.45 | 3.53 | 49.98%\\nLlama3.1-s-v2-epoch-4 | 3.47 | 2.93 | 60.05%\\nLlama3.1-s-v2-epoch-5 | 3.34 | 3.01 | 69.07%\\nOur training dataset didn‚Äôt contain Alpaca Instruction. At epoch 3, llama3-s v.02 achieved an average score of 3.53 on the ALPACA-Audio eval, which seems to beat SALMONN, Qwen-Audio and WavLLM.\\n\\nFig 3: SOTA models evaluated on AudioBench\\n\\nThe overfitting started in epoch 4. It is interesting to observe that OpenHermes-Audio eval remaining high after this epoch, likely indicative of some training data contamination. Thus we are inclined to disregard the OpenHermes-Audio criterion.\\n\\nThis checkpoint is bad at ASR, which wasn‚Äôt our target, but we included it for good measure.\\n\\nMMLU eval: Base llama3.1 has an MMLU score of 0.6380, and degrades to the following across our epochs.\\n\\n0.6380\\n\\n | MMLU | Degradation (%)\\nEpoch 1 | 0.5139 | 19.45\\nEpoch 2 | 0.4621 | 27.57\\nEpoch 3 | 0.4676 | 26.71\\nEpoch 4 | 0.4720 | 26.02\\nEpoch 5 | 0.4703 | 26.29\\nNext Steps\\n\\nLlama 3.1 v0.2 is still in its early development and has limitations:\\n\\nAdditionally, our current approach, a Type D.1‚Äâ multimodal model, has well studied limitations. Namely, there are challenges to scaling the tokenizers and a lack of fine-grained control of how modality information flows in the model. This current approach possibly requires more training data down the road as a tradeoff for its architectural simplicity.\\n\\nFor now, our next steps are as follows:\\n\\nLong term, we aim to develop an open, multi-turn speech model for llama3-s that excels in low-resource languages, with a focus on improving generalization across ASEAN‚Äôs diverse accents and dialects. Achieving this will necessitate a significant and sustained data collection effort.\\n\\nAcoustic v Semantic\\n\\ntldr: Acoustic tokens, though more rich in audio features, requires large training data and computational resources.\\n\\nThe loss on our acoustic tokens pre-training were largely stuck at 4.\\n\\n\\n\\nWhere as pretraining on semantic tokens converged to ~1.8 after 7k steps.\\n\\n\\n\\nAcknowledgements\\n\\nOpen Call\\n\\nWe‚Äôre calling on LLM researchers and audio experts to experiment with us.\\n\\nJoin the Discord fun:\\n\\nWe believe that collaborative, open research can accelerate progress in this exciting field. Whether you‚Äôre an experienced researcher or an enthusiastic newcomer, your contribution could be valuable.\\n\\nAt Homebrew Computer Company‚Äâ, we like smaller, ‚Äúedge friendly‚Äù models that are privacy preserving and feasible to train on energy-efficient clusters. Read more about our AI philosophy here‚Äâ.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/research/human-augmentation\",\n",
    "      \"raw_content\": \"On This Page\\n\\nHuman Augmentation\\n\\nMenlo is focused on human augmentation, vs. replacing humans. We see AI as the latest in a long line of tools, from fire to the wheel, that have helped humans become a sapient species.\\n\\nWe are currently experimenting with:\\n\\nGoal\\n\\nThe goal of human augmentation is to improve humans‚Äô performance and well-being.\\n\\nSkills\\n\\nWe are currently focused on designing a system to enable people to thrive by supporting critical skills 1.\\n\\nSafeguards\\n\\nWe will need to augment the user, while safeguarding 2:\\n\\nUse-cases\\n\\nWe are guided by the following use-cases:\\n\\nRookie -> Expert\\n\\nBreaking Biological Constraints\\n\\nExpanding Cognitive Capacity\\n\\nAppendix\\n\\nFootnotes\\n\\nConference? ‚Ü©\\n\\nConference? ‚Ü©\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/blog/can-llama-3-listen\",\n",
    "      \"raw_content\": \"On This Page\\n\\nBlog Authors:\\n\\nPublished August 6, 2024\\n\\nCan llama learn to listen?\\n\\nWe invite you to join llama3-s‚Äâ: an open and ongoing research experiment focused on teaching llama3 to listen. This blog post is a quick project update and an open call to join us in building a speech adapter for open-source models.\\n\\nInspired by the Chameleon‚Äâ and Llama Herd‚Äâ papers, llama3-s is an early-fusion, audio and text, multimodal model. We‚Äôre conducting this research entirely in the open, with an open-source codebase‚Äâ, open data‚Äâ and open weights‚Äâ.\\n\\nGradio Demo (19 July 2024): the model ‚Äúlistens‚Äù to audio files and does text autocompletion\\n\\nOur initial approach uses a sound compressor to create discrete token representations of sound, which are then used to train the LLM. We hypothesize that these sound tokens can be continuously learned and that existing text-based models can find semantic connections between sound and text representations.\\n\\nIn this post, we share results from a July 19th checkpoint‚Äâ. We trained llama3 8b on synthetic audio, via a small training run that cost less than $600, yielding interesting results. But this is just the beginning, and we need your expertise and ideas to push this research further.\\n\\nImage Credit: When llama learns to listen by Cary & Kumar\\n\\nThe Problem\\n\\nCascaded systems‚Äâ (currently popular in production) take user speech, transcribe it into text, and pass it into an LLM. They have been proven to be slow, inefficient, and lossy‚Äâ.\\n\\nPictured: Cascaded systems use two separate models. In addition to latency, emotion, tone, many audio features are lost.\\n\\nPictured: Multimodal models natively understand text, audio, image input.\\n\\nSignificant research in this space has been on training multimodal models that natively understand audio tokens and text tokens (see Citations).\\n\\nUsually, this entails pretraining encoders, adapters on large amounts of ASR, AST, synthetic, and human-generated datasets, making the endeavor quite data expensive and compute intensive.\\n\\nWe wanted to see how far we‚Äôd get by just continuously training llama3 on a small set of synthetic, high quality instruct QA pairs. Our initial goal is a small, scaling law experiment, where any initial convergence of audio and text tokens could have interesting implications upon larger and more diverse datasets.\\n\\nTraining: 19th July Checkpoint\\n\\nThis experiment ended up being quite achievable on a smaller budgeted run. Building on existing research, we employed a unique combination of techniques. Hopefully our initial findings below contribute meaningful learnings.\\n\\nData: For training data, we collated open datasets, consisting of 6 million (15Bn) Q&A text pairs. This was then reduced to 2Mn instruction pairs (4B tokens) through deduping, and filtering for English (FastText‚Äâ) and token length. We also used additional heuristics-based classifiers to filter out bad samples for additional quality. (See limitations).\\n\\n\\n\\nFig 1. Data tokens length distribution\\n\\nAudio generation: We then used WhisperSpeech‚Äâ text-to-speech model on the default voice tensor to generate an interleaving dataset of 2bn tokens (1Mn. wav files):\\n\\nAudio encoding: The Encodec‚Äâ VQ model was used to tokenized the audio files and create sound tokens, compressing each time step into 2D tensors, via 2 channels (24kHz audio file to 1.5 kps)  These audio tokens were then added to llama3‚Äôs existing token vocabulary. (See limitations).\\n\\nYou can find the datasets here:\\n\\nDate | HF Checkpoint | Tokens\\nüìÖ 2024-07-19 | üîóhttps://huggingface.co/homebrewltd | üî¢ 1.35B\\nüìÖ 2024-07-18 | üîóhttps://huggingface.co/datasets/homebrewltd/instruction-speech-v1.55 | üî¢ 800M\\nüìÖ 2024-06-30 | üîóhttps://huggingface.co/datasets/homebrewltd/instruction-speech-v1 | üî¢ 450M\\nTraining: The instruct tuning was done with fsdp2 (Torchtune‚Äâ) mixed-precision, on a llama3 8b‚Äâ base model, with the final weights in bf16. We used the AdamMini‚Äâ optimizer, a global batchsize of 128 (mini-batches of 2-4), a 3e-4 learning rate, and a slightly longer warm up ratio. You can find the full steps to reproduce our training here on Hugging Face‚Äâ.\\n\\n\\n\\nResults: 19 July Run\\n\\nThis checkpoint was evaluated on held-out datasets not overlapping with training data.\\n\\nLoss Evaluation: After 2 billion tokens, the model stablised at a loss of ~1.0.\\n\\nPictured: checkpoint 7-19 loss curve.\\n\\nHeuristics Evaluation: We manually evaluated this checkpoint on 100 samples:\\n\\nA good answer from the checkpoint:\\n\\n# Question (in audio tokens):\\nIs the Paris Climate Change agreement going to help slow pollution?\\n \\n# Expected Output (in text):\\nYes, the Paris Climate Agreement is designed to help slow pollution and combat climate change on a global scale. The main goal of the agreement is to limit global warming to well below 2 degrees Celsius, preferably to 1.5 degrees Celsius, compared to pre-industrial levels. To achieve this, countries that have signed the agreement commit to reducing their greenhouse gas emissions...\\n \\n# Model's Output (in text):\\nYes, the Paris Climate Agreement is a global commitment to limit global warming to well below 2 degrees Celsius above pre-industrial levels and pursue efforts to limit it to 1.5 degrees Celsius. The agreement was signed by 195 countries and entered into force on November 4, 2016. \\n \\nThe agreement sets out a framework for countries to reduce greenhouse gas emissions and adapt to the impacts of climate change. It also encourages countries to increase their ambition and to support developing countries in their efforts to reduce emissions. \\n \\nThe Paris Agreement is a significant step towards addressing climate change, but it is not a guarantee of success. The agreement requires countries to submit Nationally Determined Contributions (NDCs) outlining their plans to reduce emissions and adapt to the impacts of climate change. The success of the agreement will depend on the ability of countries to meet their NDCs and to work together to address the global nature of climate change.```\\n\\nConversely, a bad answer from the checkpoint:\\n\\n# Question (in audio tokens):\\nwhat is the address to configure a mfm harddrive on a pc without a bios\\n \\n# Expected Output (in text):\\nConfiguring an MFM (Modified Frequency Modulation) hard drive on a PC without a BIOS (Basic Input/Output System) is a complex task that involves hardware jumpers and possibly some low-level software...\\n \\n# Model's Output (in text):\\nThe average computer memory (RAM) is typically around 4GB, but it can vary depending on the specific model and manufacturer.\\n\\nMany of the bad answers were ‚Äúsound similar‚Äù hallucinations, e.g. the model mishearing Gasoline for Gaslow, which is phonetically similar.\\n\\nGasoline\\n\\nGaslow\\n\\n# Question (in audio tokens):\\nWhy is gasoline a liquid. \\n \\n# Expected Output (in text):\\nGasoline is a liquid because it is a mixture of hydrogen and oxygen...\\n \\n# Model's Output (in text):\\nGaslow is a city in Scotland...\\n\\nDegradation Evaluation: We evaluated whether the checkpoint retained original reasoning with 0 shot MMLU.\\n\\nGroups | Version | Original | New | Stderr\\nmmlu | 1 | 0.6380 | 0.2478 | 0.0036\\n- humanities | 1 | 0.5785 | 0.2504 | 0.0063\\n- other | 1 | 0.7184 | 0.2530 | 0.0078\\n- social sciences | 1 | 0.7413 | 0.2418 | 0.0077\\n- stem | 1 | 0.5468 | 0.2448 | 0.0076\\nWhile this specific checkpoint demonstrated forgetting, it has since been addressed. It was a mistake in our tuning parameters, and a 30 July checkpoint with a similar sized training set yielded 0.6137.\\n\\n0.6137\\n\\nResults:\\n\\nDate | Checkpoint | Tokens | Steps | Loss | GPU hours | All in Cost\\nüìÖ 2024-07-19 | üîóhttps://huggingface.co/homebrewltd/llama3-s-2024-07-19 | üî¢ 1.35B | üîÑ 1195k | üìâ 1.0 | 112(14x8 H100 SXM) | $530 üòÇ\\nüìÖ 2024-07-01 | üîóhttps://huggingface.co/homebrewltd/llama3-s-2024-07-08 | üî¢ 700M | üîÑ 1431k | üìâ 1.7-1.8 | 64(8x8 H100s SXM) | $280\\nüìÖ 2024-06-23 | üîóhttps://huggingface.co/homebrewltd/llama3-s-init | üî¢ 0M | üîÑ N/A | üìâ N/A | N/A | N/A\\nWe were pleasantly surprised at the initial results under such compute and data constraints. Though, it is too early to conjecture speech & text convergence.\\n\\nMinimally, we observed some transitive properties between an LLM‚Äôs existing latent text representations and newly learned audio tokens, through just instruct tuning llama3.\\n\\nHardware: for funsies, we ran our training workload cross various nodes, and measured the following.\\n\\n\\n\\nCurrent Problems\\n\\nThis 19th July checkpoint had limitations.\\n\\nNext Steps\\n\\nAt the moment, we‚Äôre working on a new iteration with the following improvements:\\n\\nHave better ideas? Join our training in public‚Äâ.\\n\\nCitations\\n\\nOpen Call\\n\\nWe‚Äôre calling on LLM researchers and audio experts to collaborate with us. As a nascent research team, Homebrew is our early days of ‚Äúthrow sh*t at the wall and see what sticks‚Äù.\\n\\nWe openly discuss training recipes in Discord, argue it out, and often run the best ideas the next day.\\n\\nJoin the Discord fun:\\n\\nWe believe that collaborative, open research can accelerate progress in this exciting field. Whether you‚Äôre an experienced researcher or an enthusiastic newcomer, your contribution could be valuable.\\n\\nAt Homebrew Computer Company‚Äâ, we like smaller, ‚Äúedge friendly‚Äù models that are privacy preserving and feasible to train on energy-efficient clusters. Read more about our AI philosophy here‚Äâ.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/blog/llama-learns-to-talk\",\n",
    "      \"raw_content\": \"On This Page\\n\\nBlog Authors:\\n\\nPublished October 4, 2024\\n\\nüçì Ichigo: Llama Learns to Talk\\n\\nHomebrew‚Äôs early-fusion speech model has evolved. Meet üçì Ichigo  - the latest llama3-s checkpoint.\\n\\nInspired by the Chameleon‚Äâ and Llama Herd‚Äâ papers, llama3-s (Ichigo) is an early-fusion, audio and text, multimodal model. We‚Äôre conducting this research entirely in the open, with an open-source codebase‚Äâ, open data‚Äâ and open weights‚Äâ.\\n\\nImage generated by ChatGPT\\n\\nDemo\\n\\nA real-time demo of üçì¬†Ichigo (7th Oct 2024 checkpoint): the MLLM listens to human speech and talks back.\\n\\nüçì¬†Ichigo\\n\\nYou can try it for yourself:\\n\\n*Inference may slow/queued due to shared compute on a single NVIDIA RTX4090\\n\\nThis post shares methodology and results behind this latest checkpoint. As always, this is just the beginning, and we need your ideas to push this research further.\\n\\nChangelog\\n\\nFrom the llama3-s-v0.2 checkpoint‚Äâ, we identified several areas for improvement:\\n\\nIchigo addresses these limitations through a three-phase training approach.\\n\\nTraining\\n\\nPhase 1: Continual Pre-training on Multilingual Speech\\n\\nData: In this phase we shifted from an English-only dataset‚Äâ to 7 languages dataset‚Äâ. This helps align the model‚Äôs distribution more closely with the original multilingual training of the base LLM.\\n\\nFig 1. Synthetic data generation pipeline\\n\\nTokenizer: To accommodate our shift towards a multilingual dataset, we made a change in our tokenizer from English-only checkpoint‚Äâ to 7 languages checkpoint‚Äâ.\\n\\nTraining: The pre-training totaled 8064 steps and took over 45 hours on 10xA6000s. We used¬†Torchtune‚Äôs‚Äâ¬†Fully Sharded Data Parallels 2 (FSDP2), an AdamW Fused optimizer, along with the following parameters:\\n\\nParameter | Continual Training\\nEpoch | 1\\nGlobal batch size | 480\\nLearning Rate | 2e-4\\nLearning Scheduler | LambdaLR with warmup\\nOptimizer | AdamW Fused\\nWarmup Steps | 50\\nWeight Decay | 0.005\\nMax length | 512\\nPrecision | bf16\\nLoss: Training loss converged to just below 2. This loss convergence pattern is similar to what we observed in our previous run‚Äâ.\\n\\n\\n\\nMMLU Eval: We measured MMLU at this stage to get a sense of degradation. 5-shot MMLU dropped from 0.69‚Äâ ‚Üí 0.42 This decrease rate is higher than our previous run.\\n\\nPhase 2:¬†Balancing Original Performance and Speech Modality\\n\\nThis phase focused on recovering the model‚Äôs general capabilities while enhancing its speech-related skills\\n\\nAddressing MMLU Performance Drop: The continual pre-training in Phase 1, while necessary for introducing speech capabilities, significantly reduced the model‚Äôs original performance. This is a common challenge when retraining a pre-trained model on new vocabulary. Our goal was to recover these capabilities without compromising the newly acquired speech understanding.\\n\\nSpoiler alert: We recovered MMLU performance from 0.42 to 0.63, reducing the degradation rate to approximately 10%.\\n\\nOptimizing Data and Training Strategies\\n\\nData:\\n\\nTranscription token: Previously, we used 513 semantic tokens from WhisperVQ‚Äôs codebook, 2 special tokens for sound input boundaries, and 1 special token for transcription tasks. However, we discovered that the transcription token hindered model recovery.\\n\\nOur solution:\\n\\nTest Name | Pretrain Checkpoint | Dataset | SpeechQA data | Instruction-text data | Transcription data | Final MMLU Score\\nTest 1: Early Pretrain Recovery | 3,000 steps | 500k mixed | ‚úÖ | ‚úÖ | ‚ùå | 0.55\\nTest 2: Late Pretrain Recovery | Last | 500k mixed | ‚úÖ | ‚úÖ | ‚ùå | 0.515\\nTest 3: Late Pretrain Recovery with Transcription (With transcription token) | Last | 500k mixed | ‚úÖ | ‚úÖ | ‚úÖ | 0.48\\nTest 4: Extended Late Pretrain Recovery (With transcription prompts) | Last | 1.89M mixed | ‚úÖ | ‚úÖ | ‚úÖ | 0.63\\nMixed training data between modalities: We determined an optimal interleaving of different data types with 70% speech instruction prompts, 20% speech transcription prompts and 10% text-only prompts.\\n\\n\\n\\nThis distribution was not arrived at arbitrarily. We conducted several permutation tests to find the sweet spot that balances speech understanding, transcription abilities, and general language skills.\\n\\nTraining: The phase 2 training totaled 7400 steps and took over 10 hours on 8xH100s with the following parameters:\\n\\nParameter | Continual Training\\nEpoch | 1\\nGlobal batch size | 256\\nLearning Rate | 7e-5\\nLearning Scheduler | LambdaLR with warmup\\nOptimizer | AdamW Fused\\nWarmup Steps | 73\\nWeight Decay | 0.005\\nMax length | 4096\\nPrecision | bf16\\nLoss: Training loss converged at 1.08\\n\\n\\n\\nPhase 3: Teach Ichigo To Say ‚ÄúI cannot hear‚Äù\\n\\nIn this final phase, we focused on fine-tuning the model to improve user interaction, particularly in handling inaudible inputs and multi-turn conversations.\\n\\nObjectives\\n\\nTeach model to say ‚ÄúI cannot hear‚Äù\\n\\nOur initial approach was to create a synthetic dataset of random environmental noises. However, we quickly realized that this method was difficult to scale.\\n\\nTo fix this problem, we hypothesized that meaningful speech typically follows certain patterns. This led us to utilize the 513 sound tokens from the WhisperVQ codebook, and randomize them into similar patterned sequences. This method allowed us to generate a vast amount of ‚Äúinaudible‚Äù input data with a wide distribution. The key insight was that if the model could recognize these chaotic patterns as inaudible input, it would learn to decline responses appropriately.\\n\\nInaudible space is much larger than Speech space\\n\\nTo put this in perspective: With an average speech input of about 50 sound tokens, there are 513^50 possible arrangements. However, only a tiny fraction of these arrangements would constitute meaningful speech. By exposing our model to a wide range of these chaotic arrangements, we taught it to distinguish between audible and inaudible inputs effectively.\\n\\nData:\\n\\nMulti-turn Conversation:\\n\\nTo enhance multi-turn capabilities with speech input, we fine-tuned the model using 150K samples. The dataset composition was:\\n\\n\\n\\nInaudible Refusal data:\\n\\nFor the inaudible inputs, we employed the Qwen2.5-72B model‚Äâ to generate diverse synthetic answers. This process was facilitated through Distillabel‚Äâ, ensuring a wide range of appropriate ‚Äúdecline‚Äù responses.\\n\\nBeyond randomizing sound tokens for inaudible input, we also performed sequence length distribution matching between inaudible and audible data. This ensured a balanced representation of both types of inputs in our training set.\\n\\nTraining: The phase 3 training totaled 644 steps and took over 3 hours on 8xH100s with the following parameters:\\n\\nParameter | Continual Training\\nEpoch | 1\\nGlobal batch size | 256\\nLearning Rate | 1.5e-5\\nLearning Scheduler | LambdaLR with warmup\\nOptimizer | AdamW Fused\\nWarmup Steps | 8\\nWeight Decay | 0.005\\nMax length | 4096\\nPrecision | bf16\\nLoss: Loss converged at 0.98\\n\\n\\n\\nResults\\n\\nMMLU: We recovered the MMLU after phase 1 and maintained the instruct following performance after phase 3\\n\\n\\n\\nWe know that with only MMLU is insufficient to claim recovery. So we will also benchmark on more text-based datasets in our upcoming research paper.\\n\\nAudioBench Eval: AudioBench‚Äâ¬†is a June 2024 benchmark designed to evaluate audio large language models (AudioLLMs). It measures speech capabilities, in addition to ASR, transcription, etc., through a compilation of many open datasets.\\n\\nModel Bench | Open-hermes Instruction Audio(GPT-4-O judge 0:5) | Alpaca Instruction Audio(GPT-4-O judge 0:5)\\nLlama3.1-s-v2 | 3.45 | 3.53\\nIchigo-llama3.1-s v0.3-phase2 -cp7000 | 3.42 | 3.62\\nIchigo-llama3.1-s v0.3-phase2-cplast | 3.31 | 3.6\\nIchigo-llama3.1-s v0.3-phase3 | 3.64 | 3.68\\nQwen2-audio-7B | 2.63 | 2.24\\nNext steps\\n\\nüçì¬†Ichigo is still in early development and has limitations:\\n\\nFor now, our next steps are as follows:\\n\\nLong term, we aim to develop üçì¬†Ichigo as a production-level tool that can be integrated in your AI applications.\\n\\nAppendix\\n\\nData distribution:\\n\\nTask Type | v0.2 | v0.3\\nSpeech Multi-turn | None | 140K samples: 2 turns, 10K samples >= 4 turns\\nSpeech QA | 679K samples | 1.33M samples\\nTranscription | 250K samples (Using a special token) | 400K samples (6 different prompts)\\nNoise Audio | None | 8K samples\\nText-only | None | 100K samples: multi-turn, 50K samples: single turn\\nPrompts used for transcription data\\n\\nTranscribe the following audio clip: <speech>\\nConvert the spoken words to text: <speech>\\nWhat is being said in this audio clip: <speech>\\nTranscribe the speech in this audio sample: <speech>\\nPlease write down what is being said in the audio clip: <speech>\\nGenerate a transcript from this sound file: <speech>\\nRecognize the speech in this audio clip: <speech>\\nProduce a text version of this audio recording: <speech>\\n\\nAcknowledgements\\n\\nOpen Call\\n\\nWe‚Äôre calling on LLM researchers and audio experts to experiment with us.\\n\\nJoin the Discord fun:\\n\\nWe believe that collaborative, open research can accelerate progress in this exciting field. Whether you‚Äôre an experienced researcher or an enthusiastic newcomer, your contribution could be valuable.\\n\\nAt¬†Homebrew Computer Company‚Äâ, we like smaller, ‚Äúedge friendly‚Äù models that are privacy preserving and feasible to train on energy-efficient clusters. Read more about our¬†AI philosophy here‚Äâ.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/blog/alpha-maze\",\n",
    "      \"raw_content\": \"On This Page\\n\\nBlog Authors:\\n\\nPublished February 13, 2025\\n\\nAlphaMaze: Teaching LLMs to think visually\\n\\nAlphaMaze is a decoder-only LLM designed to solve visual reasoning tasks that traditional language models struggle with on ARC-AGI benchmark. It tackles maze puzzles‚Äîtasks that humans solve intuitively but LLMs typically fail‚Äîthrough targeted visual decoding techniques.\\n\\nIntroduction\\n\\nRecent breakthroughs in large language models (LLMs) have demonstrated a fascinating concept: giving models time to think dramatically improves their performance. These ‚Äúreasoning‚Äù models, as they‚Äôre often called, have achieved remarkable results on complex mathematical benchmarks like AIME‚Äâ and MATH‚Äâ. This success isn‚Äôt limited to massive models like O3-mini and Deepseek-r1; the community has also achieved strong results by fine-tuning smaller, ‚Äúdistilled‚Äù models, such as 1.5b‚Äâ and 7b‚Äâ versions.\\n\\nThe core algorithm driving much of this progress is GRPO‚Äâ (Group Relative Policy Optimization). GRPO is an online learning method, meaning it constantly refines the model using data generated during the training process itself. It‚Äôs a powerful way to iteratively improve performance.\\n\\nHowever, while these reasoning models excel in text-based tasks like solving mathematical problems, they often struggle with tasks that require visual understanding. For example, benchmarks like ARC-AGI, which involve visual reasoning, remain challenging for these models, even though they might seem straightforward to a human.\\n\\nInspired by the spirit of the ARC-AGI test, we train the model on a maze-solving task‚Äîsomething intuitive for humans but challenging for AI. Our goal is to enhance the visual reasoning capabilities of AI by focusing on a task that requires spatial understanding and step-by-step problem-solving.\\n\\nDemo\\n\\nThe video demonstrates AlphaMaze‚Äôs process: inputting a maze, the model will reason through and output of the most likely direction.\\n\\nOverview\\n\\nPrior research, such as Microsoft‚Äôs ‚ÄúMultimodal Visualization-of-Thought (MVoT)‚Äâ‚Äù, has shown that models can solve image-based mazes by generating visual representations of their reasoning process. However, these models still have limitations in their ability to create accurate visual representations and are often evaluated using multiple-choice question (MCQ) setups, which may not fully capture their reasoning abilities.\\n\\nAlphaMaze takes a different approach. We built our model on a distilled, 1.5-billion parameter Qwen model‚Äâ (based on Deepseek-R1), and trained it to solve mazes presented in a text-based format. Our core hypothesis is that if a model can successfully reconstruct a maze from a text description and use that internal representation to plan its next move, it demonstrates a form of visual reasoning.\\n\\nNormal LLM\\n\\n\\n\\nGPT-4o‚Äâ and Qwen-Max‚Äâ failed on solving a maze.\\n\\nCurrent language models exhibit consistent limitations in maze navigation tasks, indicating a gap in spatial reasoning capabilities.\\n\\nAlphaMaze\\n\\n\\n\\nVisualization of AlphaMaze‚Äôs step-by-step reasoning process while solving a maze. Each step shows the model‚Äôs thought process and directional decision.\\n\\nCorrect answer: <|down|><|left|><|left|><|down|><|right|><|right|><|right|>\\n\\n<|down|><|left|><|left|><|down|><|right|><|right|><|right|>\\n\\nTraining\\n\\nOur training process involved two key stages: creating a specialized dataset and then using a combination of supervised fine-tuning (SFT) and reinforcement learning (RL) to train the model.\\n\\nCreating the Maze Dataset\\n\\nTo train a model to reason about mazes, we needed a large and diverse dataset of mazes.  We leveraged the excellent maze-dataset framework‚Äâ from Understanding Search, which generates 5x5 mazes using a Depth-First Search (DFS) algorithm. This provided us with a solid foundation of varied maze layouts.\\n\\nHowever, we needed to transform these mazes into a format that our language model could understand.  This is where our token-based representation system comes in.  Think of it as translating the visual structure of the maze into a ‚Äúlanguage‚Äù the model can process.\\n\\nHere‚Äôs how we broke it down:\\n\\nSpatial relationships are encoded through a token system: <row-col> for coordinates, directional markers for walls, and <up>, <down>, <left>, <right> for movements.\\n\\n<row-col>\\n\\n<up>\\n\\n<down>\\n\\n<left>\\n\\n<right>\\n\\nThe key innovation is our token system. We use special tokens to represent:\\n\\n<row-col>\\n\\n<0-0>\\n\\n<2-4>\\n\\n<no_wall>\\n\\n<up_wall>\\n\\n<down_wall>\\n\\n<up>\\n\\n<down>\\n\\n<left>\\n\\n<right>\\n\\n<origin>\\n\\n<target>\\n\\nThis system enables us to generate step-by-step navigation sequences, where each step captures the entire maze state along with a single directional indicator. This structured representation makes it well-suited for training models to reason about movement and spatial decision-making.\\n\\nAs a result, we have compiled a dataset of 100,000 maze examples, available here: Maze-Reasoning Dataset‚Äâ.\\n\\nImplementation Details:\\n\\nThe token generation process is essentially a conversion of the maze structure into a carefully ordered sequence of these tokens. This sequence provides a complete, step-by-step representation of the maze, suitable for training models on visual reasoning and pathfinding tasks.\\n\\nStage 1: Supervised Fine-Tuning\\n\\nGoals:\\n\\nMethodology:\\n\\nWe explored two approaches:\\n\\nSFT Hyperparameters:\\n\\nTraining data: 500,000 samples (filter out samples have over 4096 tokens).\\n\\nParameter | Value\\nmodel_name_or_path | deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\\ndisable_gradient_checkpointing | true\\nfinetuning_type | full\\ndeepspeed | ds0\\ncutoff_len | 4096\\ntrain_on_prompt | true\\nper_device_train_batch_size | 32\\ngradient_accumulation_steps | 1\\nlearning_rate | 1.0e-5\\nnum_train_epochs | 1.0\\nlr_scheduler_type | cosine\\nwarmup_ratio | 0.1\\nbf16 | true\\nmodel_name_or_path\\n\\ndeepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\\n\\ndisable_gradient_checkpointing\\n\\ntrue\\n\\nfinetuning_type\\n\\nfull\\n\\ndeepspeed\\n\\nds0\\n\\ncutoff_len\\n\\n4096\\n\\ntrain_on_prompt\\n\\ntrue\\n\\nper_device_train_batch_size\\n\\n32\\n\\ngradient_accumulation_steps\\n\\n1\\n\\nlearning_rate\\n\\n1.0e-5\\n\\nnum_train_epochs\\n\\n1.0\\n\\nlr_scheduler_type\\n\\ncosine\\n\\nwarmup_ratio\\n\\n0.1\\n\\nbf16\\n\\ntrue\\n\\nStage 2: Reinforcement Learning (RL) with GRPO\\n\\nAfter SFT, we used GRPO to further refine the model‚Äôs maze-solving abilities.  GRPO is particularly well-suited for this task because it allows the model to learn from its own generated solutions, iteratively improving its strategy.\\n\\nConstructing the GRPO Reward Function\\n\\nA crucial aspect of reinforcement learning is the reward function, which guides the model towards desired behaviors. We designed a composite reward function with several components, each focusing on a different aspect of good maze-solving:\\n\\nCorrectness\\n\\n</think>\\n\\nThe reward function implements a dual-weighted system: solution accuracy (2.0) and move validity (0.5), optimizing for both correctness and constraint satisfaction.\\n\\nValidity:\\n\\nValidity\\n\\n<up>\\n\\n<down>\\n\\n<left>\\n\\n<right>\\n\\nStrict Formating:\\n\\nStrict Formating\\n\\n^<think>\\\\n.*?\\\\n</think>\\\\n\\\\n.*?\\\\n$\\n\\n<think>\\n\\n</think>\\n\\nSoft Formating:\\n\\nSoft Formating\\n\\n<think>.*?</think>\\\\s*.*?\\n\\n<think>\\n\\n</think>\\n\\nTag Formating:\\n\\nTag Formating\\n\\n<think>\\\\n\\n\\n\\\\n</think>\\\\n\\n\\nTotal Reward Calculation:\\n\\nThe final reward is the sum of all these components:\\n\\nTotal Reward = Correctness + Validity + Strict Formating + Soft Formating + Tag Formating\\n\\nMaximum Possible Reward: 2.0 + 0.5 + 0.5 + 0.5 + 0.25 = 3.75\\n\\nGRPO training hyperameters\\n\\nFor the GRPO training, we configured the following parameters and models:\\n\\nWe use Unsloth framework.\\n\\nParameter | Value\\nlearning_rate | 1e-6\\nalpha | 128\\nr | 128\\nweight_decay | 0.1\\nwarmup_ratio | 0.1\\nlr_scheduler_type | cosine\\noptim | paged_adamw_8bit\\nper_device_train_batch_size | 8\\ngradient_accumulation_steps | 1\\nnum_generations | 4\\nmax_prompt_length | 612\\nmax_completion_length | 4096\\nmax_grad_norm | 0.1\\nResults\\n\\nSFT\\n\\nWe use llama-factory to SFT the model\\nHere‚Äôs a summary of the SFT training runs:\\n\\nRun ID | Model Config | Dataset | Steps | Loss | Hardware | Detailed Explanation\\nexp-1 | Full-Finetune | Maze Reasoning | 3125 | 0.01 | ~1.5 hours on 6xA6000s | Added new maze tokens\\nexp-2 | Full-Finetune | Maze Reasoning | 3125 | 0.01 | ~1.5 hours on 6xA6000s | Pure text\\nexp-3 | Full-Finetune | Maze Reasoning | 2344 | 0.0077 | ~12 hours on 6xA6000s | Expanded pure text training\\nexp-4 | Full-Finetune | Maze Reasoning | 2344 | ~0 | ~1.5 hours on 6xA6000s | Expanded pure text training\\nexp-5 | Full-Finetune | Maze Reasoning | 3681 | ~0.02 | ~1 hours on 8xH200s | Added new maze tokens\\nAdding new maze tokens results in poor performance. However, without the additional tokens, the model performed surprisingly well with the same training steps.\\n\\nExperimental results suggest reduced token complexity correlates with improved spatial-to-language translation performance.\\n\\nGRPO\\n\\nFigure below displays the MazeBench scores (blue crosses) over GRPO steps along with a linear regression trendline (red dashed line). The steady increase in the trendline indicates that GRPO effectively guides the model towards improved maze-solving policies.\\n\\n\\n\\nKey Takeaways\\n\\nImplications\\n\\nThis research demonstrates the potential for enhancing the visual thinking capabilities of language models using a text-based approach.  If a text-based model can successfully reason about and solve mazes, it suggests an internal understanding of spatial relationships and visual concepts.  This opens up exciting possibilities:\\n\\nNext Steps\\n\\nModel Links\\n\\nAcknowledgements\\n\\nAppendix\\n\\nPrompt for instruction model training (SFT and GRPO)\\n\\nYou are a helpful assistant that solves mazes.  You will be given a maze represented by a series of tokens.\\nThe tokens represent:\\n- Coordinates: `<row-col>` (e.g., `<0-0>`, `<2-4>`)\\n- Walls:  `<no_wall>`, `<up_wall>`, `<down_wall>`, `<left_wall>`, `<right_wall>`, `<up_down_wall>`, etc.\\n- Origin: `<origin>`\\n- Target: `<target>`\\n- Movement: `<up>`, `<down>`, `<left>`, `<right>`, `<blank>`\\n\\nYour task is to output the sequence of movements (`<up>`, `<down>`, `<left>`, `<right>`) required to navigate from the origin to the target, based on the provided maze representation.  Think step by step. At each step, predict only the next movement token. Output only the move tokens, separated by spaces.\\nMAZE:\\n{maze_prompt}\\n\\nOpen Call\\n\\nWe‚Äôre calling on researchers to experiment & build-in-public with us. Join the #research‚Äâ in Discord.\\n\\nWe believe that collaborative, open research can accelerate progress in this exciting field. Whether you‚Äôre an experienced researcher or an enthusiastic newcomer, your contribution could be valuable.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/philosophy/humanity\",\n",
    "      \"raw_content\": \"On This Page\\n\\nHumanity-aligned\\n\\n‚ÄúThe imagined community of the nation is always limited and sovereign, yet human connections and cooperation stretch far beyond borders.‚Äù ‚Äî Benedict Anderson\\n\\nWhy this matters\\n\\nWho we are\\n\\nMenlo‚Äôs stance\\n\\nThe bigger picture\\n\\nHuman history is one of scaling cooperation:\\n\\nHistory doesn‚Äôt progress. It moves‚Äîmessy, cyclical, and full of contradiction\\n\\nTechnology changes the terrain:\\n\\nAI is another fork in the road. It can reinforce old hierarchies or dismantle them. It can be used to surveil or to liberate\\n\\nWhy we exist\\n\\nOur vision\\n\\nIf our work helps the world coordinate better‚Äîeven slightly‚Äîwe‚Äôve done something that matters\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/why/problem\",\n",
    "      \"raw_content\": \"On This Page\\n\\nWhat problem are we solving?\\n\\n\\n\\nMost valuable work is still out of reach for today‚Äôs AI:\\n\\nWhat‚Äôs needed is general intelligence: a system that can see, hear, think, and act across domains, online and offline, digital and physical.\\n\\nMission\\n\\nTo build technology that lets you do extraordinary things.\\n\\nApproach\\n\\nWe solve this incrementally. We productize sub-problems that can be solved now‚Äâ, these become key components of our long-term vision.\\n\\nWe earn the right to build our long-term vision by delivering value to users today.\\n\\nVision\\n\\nWe believe general intelligence can, and must, be built in service of people:\\n\\nWe‚Äôre optimistic about what comes next if we build carefully, collaboratively, and out in the open.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/lifecycle/interviewing\",\n",
    "      \"raw_content\": \"On This Page\\n\\nInterviewing\\n\\nOur talent pipeline is an organizational moat. This guide walks you through our interview process.\\n\\nProcess\\n\\nWe adopt a self-service interview process.\\n\\nWe take a data-driven approach, and each open role is tracked in our automated tracking system. The data is used to improve our hiring process.\\n\\nHistorically, our acceptance rate has been less than 1% of all applications. For direct referrals, this rate is higher at >10%.\\n\\nPreparing Our Interviewers\\n\\nTo improve the effectiveness of the interview process, we actively support our team members in stepping up as hiring managers. Any team member that is keen to participate in the interview process is encouraged to follow these steps:\\n\\nEvaluation\\n\\nWe do not use a scoring system for candidates; instead we prefer to encourage natural conversation.\\n\\nHowever interviewers who like to use an evaluation template can consider the following:\\n\\nCriteria | Description | Response | Notes\\nTechnical Skills | Proficiency in relevant technical areas (e.g. AI, robotics, programming) |  | \\nFundamentals | Understanding of core concepts in the candidate‚Äôs field of expertise (e.g. statistics, mathematics, machine learning, physics) |  | \\nProblem-Solving Ability | Ability to approach and solve complex problems |  | \\nCommunication Skills | Clarity and effectiveness in communication |  | \\nCultural Fit | Alignment with companycultureandvalues |  | \\nOverall Impression | Holistic evaluation of the candidate‚Äôs suitability for the role applied |  | \\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/sell/marketing\",\n",
    "      \"raw_content\": \"On This Page\\n\\nMarketing\\n\\n‚ÄúMarketing is about values.‚Äù - Steve Jobs\\n\\nWithout a strategy, marketing is:\\n\\nMarketing is telling your story through channels for a purpose. Without a purpose, it‚Äôs just noise - like how some do.\\n\\nBefore starting we should align on some terms:\\n\\nFramework(s)\\n\\nMarketing looks like art, must be managed like math. At Menlo, we follow some frameworks for alignment.\\n\\nOur marketing efforts consist of 3 purposes and each marketing action must target at least one:\\n\\nEach purpose requires campaigns with clear goals. Goal types:\\n\\nCampaign executions must leave no questions, so each marketing campaign requires a brief format:\\n\\nPositioning\\n\\nMarketing starts with positioning, we always think thoroughly about where to sit in the market before new launches.\\n\\nNo one cares about product functions, it‚Äôs all about what you provide. If your positioning requires explanation, it isn‚Äôt working. We never talk about what our product does until we‚Äôve established what problem it eliminates.\\n\\nWe start with a positioning:\\n\\nWhat to avoid in marketing\\n\\nWe‚Äôre playing our game, not others‚Äô.\\n\\nWhat to embrace in marketing\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/lifecycle/onboarding\",\n",
    "      \"raw_content\": \"On This Page\\n\\nOnboarding\\n\\nWelcome to Menlo! We are excited to have you join our team. This guide walks you through the onboarding process.\\n\\nYou will receive an onboarding invitation on the morning of your first day. Prior to the scheduled onboarding call, please send your Github and Discord handles to [email¬†protected].\\n\\n[email¬†protected]\\n\\nExpectations\\n\\nWe are a remote team, so we find it important to establish clear expectations.\\n\\nExpectation | Description\\nTake Initiative | Take ownership of an area. If you see a problem, raise it to the team and own it to completion. Your work will often not be defined. You would be expected to take the initiative to figure out what needs to be done, seek clarification and communicate your plan to the team.\\nBias to Action | There are many problems to solve. Don‚Äôt wait for permission or try to build consensus - just take action. We expect you to be able to show clear results from turning around one or more problematic areas after a few months.\\nCommunication | Clear and timely communication is key. If you are unsure, don‚Äôt be afraid to ask questions. We are a global team and mutual respect is paramount. We do not tolerate disrespectful behavior, and we want you to focus on problem solving, not personal attacks. We abide by our variant of Hanlon‚Äôs razor, i.e. ‚Äúnever attribute to malice that which is adequately explained by lack ofstupiditycommunication or ambiguity‚Äù.\\nMastery | In this nascent industry, expertise comes from doing. We want you to own your area of expertise and build mastery.\\nCode of Conduct\\n\\n#hr-NAME\\n\\nTools\\n\\nAs much as possible, we build in public and use the following tools to collaborate asynchronously.\\n\\nChecklists\\n\\nDay 1\\n\\n#welcome\\n\\nVPN access instructions\\n\\n#hr-NAME\\n\\nImport Who‚Äôs out (on leave) calendar from BambooHR\\n\\nWeek 1\\n\\n#daily-updates\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/products/labs/sim-lim\",\n",
    "      \"raw_content\": \"On This Page\\n\\nSim Lim Square\\n\\n\\n\\nMenlo‚Äôs first lab is in Sim Lim Square in Singapore.\\n\\nSim Lim Square\\n\\nSim Lim Square‚Äâ is the most established electronics complex in Singapore, a landmark where generations of computer enthusiasts have come, and where soldering irons, RAM chips and motherboards are sold.\\n\\nWe‚Äôve chosen Sim Lim because it personifies Menlo‚Äôs culture:\\n\\nUnfortunately, Sim Lim Square has been declining in recent years. We hope to revive it by bringing in garage energy and a new generation of builders.\\n\\nWhat‚Äôs Inside\\n\\nCoworking Space\\n\\nWe have a small coworking space with a few desks and workstations, which functions as a meeting space for our fully remote team when they work from Singapore.\\n\\nRobotics Lab\\n\\nThis is where our Robots live, learn, and evolve:\\n\\nMakerspace\\n\\nWe have a small makerspace with a growing collection of tools:\\n\\nBooks\\n\\nWe have a small collection of books on robotics, AI, and machine learning.\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/money/how\",\n",
    "      \"raw_content\": \"On This Page\\n\\nHow we make money\\n\\nIn a great market ‚Äî a market with lots of real potential customers ‚Äî the market pulls product out of the startup.\\n\\n‚Äî Marc Andreessen\\n\\nOur Markets\\n\\nWe focus on markets where these breakthroughs are just beginning:\\n\\nMarket | 2025 | 2032 | Growth\\nAI Agents | $3.7B | $104B | 44.9%\\nEdge AI Hardware | $27B | $270B | 33.3%\\nDatacenter AI Hardware1 | $142B | $837B | 34.3%\\nGPU Cloud | $6B | $74B | >35%\\nHyperscaler2 | $231.2B | $2.1T | 37.8%\\nGenerative AI | $40B | $1.3T | 42%\\nRobotics | $90B | $350B | 15.2%\\nThese markets represent massive potential. Even a small slice‚Äîjust 0.01%‚Äîwould mean $213.4M in revenue by 2032.\\n\\nBut the real opportunity is in creating the value.\\n\\nValue Creation vs Value Capture\\n\\n‚ÄúA platform is when the economic value of everybody that uses it, exceeds the value of the company that creates it. Then it‚Äôs a platform.‚Äù ‚Äî Bill Gates\\n\\nWe build for creating value:\\n\\nThis creates a virtuous cycle:\\n\\nBeing Frugal\\n\\nSuccess equation: Revenue minus Costs. We‚Äôre good at the second part.\\n\\nConstraints breed innovation:\\n\\nMenlo started as an open source project, with very little money. This has shaped our DNA:\\n\\nThe foundation of our perspective is Lateral Thinking.\\n\\nSources\\n\\nFootnotes\\n\\nGlobalNewsWire: AI Server Market‚Äâ ‚Ü©\\n\\nFortune Business Insights: Hyperscaler Market‚Äâ ‚Ü©\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/how/discord\",\n",
    "      \"raw_content\": \"On This Page\\n\\nInternal Tools\\n\\nWe like to make AI do the boring stuff. We also automate and dogfood our own tools as much as possible.\\n\\nTools\\n\\nInfrastructure\\n\\nWe use a mix of cloud providers and build our own on-premises hardware to cut costs and optimize performance.\\n\\nSee Infra for more details.\\n\\nCI/CD\\n\\nWe have a dedicated devops/mlops team. Ping in the infra-internal channel for any requests.\\n\\ninfra-internal\\n\\nAutomations\\n\\nWe have a dedicated automations engineer. Her goal is to automate yesterday‚Äôs tasks today, so that you are doing something different tomorrow.\\n\\nPing in the #automations channel for any requests.\\n\\n#automations\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    },\n",
    "    {\n",
    "      \"url\": \"https://menlo.ai/handbook/lifecycle/offboarding\",\n",
    "      \"raw_content\": \"On This Page\\n\\nOffboarding\\n\\nWe are grateful for the time you have spent with us. Regardless of your next endeavor, whether that may be starting your own company or pursuing your life‚Äôs work, we will be cheering you on from the sidelines and hope you continue to remain part of the Menlo community.\\n\\nTo ensure a seamless transition, we have outlined the offboarding process below.\\n\\nNotice Period\\n\\nFor voluntary departures past the probation period, we usually like you to serve your notice to ensure a smooth transition for the team. Nevertheless we understand that this may not always be possible and will work with you to find a solution.\\n\\nFinal Pay\\n\\nFinal pay and reimbursements will be processed at the end of the month of your departure. Our finance team will inform you on the breakdown of your final pay.\\n\\nFor any vested options you currently hold, there is usually a prescribed period to exercise them on departure. If you are not in a position to exercise them within that period, we can extend that period for good leavers, as a gesture of goodwill. This is however at the discretion of the board of directors, and not guaranteed.\\n\\nChecklist\\n\\nThe Soul of a New Machine\\n\\nSubscribe to our newsletter on AI research.\\n\\nProducts\\n\\nModels\\n\\nInfra\\n\\nCommunity\\n\\n\",\n",
    "      \"images\": []\n",
    "    }\n",
    "  ],\n",
    "  \"response_time\": 6.63\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
